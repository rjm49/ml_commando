{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambridge ML Commando Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition with Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this notebook, we show how to perform face recognition using Support Vector Machines. We will use the Olivetti faces dataset, included in Scikit-learn. More info at:_ http://scikit-learn.org/stable/datasets/olivetti_faces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the olivetti faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import bz2\n",
    "import pickle\n",
    "\n",
    "# fetch the faces data ... annoyingly this seems to not work in later (>0.20) versions of sklearn\n",
    "# so as a backup we have a local version of the dataset\n",
    "# faces = fetch_olivetti_faces()\n",
    "\n",
    "# with bz2.BZ2File(\"faces.pbz2\", \"w\") as f: \n",
    "#     pickle.dump(faces, f)\n",
    "\n",
    "f = bz2.BZ2File(\"./data/faces.pbz2\", \"rb\")\n",
    "faces = pickle.load(f)\n",
    "print (faces.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data, faces.images has 400 images of faces, each one is composed by a matrix of 64x64 pixels.\n",
    "faces.data has the same data but in rows of 4096 attributes instead of matrices (4096 = 64x64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (faces.keys())\n",
    "print (faces.images.shape)\n",
    "print (faces.data.shape)\n",
    "print (faces.target.shape)\n",
    "\n",
    "np.set_printoptions(precision=2, threshold=math.inf) #rjm49 ... lets us print a face array out in full\n",
    "print(\"What are we actually dealing with here?...\")\n",
    "eg = faces.data[0]\n",
    "print(\"initial shape is\", eg.shape)\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "sidelen = int(numpy.sqrt(4096))\n",
    "eg = eg.reshape((sidelen, sidelen))\n",
    "print(\"have made the example square of sidelen\", sidelen)\n",
    "plt.imshow(eg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to scale attributes, because data is already normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.max(faces.data))\n",
    "print (np.min(faces.data))\n",
    "print (np.mean(faces.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_faces(images, target, min_n=0, top_n=20, faces_across=20):\n",
    "    # set up the figure size in inches\n",
    "    if top_n > len(images):\n",
    "        top_n = len(images)\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    for i in range(min_n, top_n):\n",
    "        # plot the images in a matrix of 20x20\n",
    "        p = fig.add_subplot(faces_across, faces_across, i + 1, xticks=[], yticks=[])\n",
    "        p.imshow(images[i], cmap=plt.cm.bone)\n",
    "        \n",
    "        # label the image with the target value\n",
    "        p.text(0, 14, str(target[i]), color='red') # this is the Person ID at the top\n",
    "        p.text(0, 60, str(i), color='red') # this is the Image ID at the bottom\n",
    "    plt.show()\n",
    "        \n",
    "def faces_by_index(images, select):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    for i,s in enumerate(select):\n",
    "        p = fig.add_subplot(5,5, i+1, xticks=[], yticks=[])\n",
    "        p.imshow(images[s], cmap=plt.cm.bone)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_faces(faces.images, faces.target, top_n=20, faces_across=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the faces in a matrix of 20x20, for each one, we'll put it target value in the top left corner and it index in the bottom left corner.\n",
    "It may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_faces(faces.images, faces.target, top_n=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to build a classifier whose model is a hyperplane that separates instances (points) of one class from the rest. Support Vector Machines (SVM) are supervised learning methods that try to obtain these hyperplanes in an optimal way, by selecting the ones that pass through the widest possible gaps between instances of different classes. New instances will be classified as belonging to a certain category based upon which side of the surfaces they fall. Let's import the SVC class from the sklearn.svm module. SVC stands for Support Vector Classifier: we will use SVM for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a linear kernel: http://en.wikipedia.org/wiki/Kernel_%28linear_algebra%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_1 = SVC(kernel='linear') # default kernel=\"rbf\"\n",
    "print (svc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        faces.data, faces.target, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold cross validation iterator\n",
    "#     cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=K)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(svc_1, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print (\"Accuracy on training set:\")\n",
    "    print (clf.score(X_train, y_train))\n",
    "    print (\"Accuracy on testing set:\")\n",
    "    print (clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print (\"Classification Report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure precision and recall on the evaluation set, for _each class_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(svc_1, X_train, X_test, y_train, y_test)\n",
    "# train_and_evaluate(linclf, X_train, X_test, y_train, y_test)\n",
    "# train_and_evaluate(knn, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glasses or Not-Glasses?\n",
    "Performace on face recognition is very good. Now, another problem: let's try to classify images of people with and without glasses. By hand, we have marked people with glasses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index ranges of images of people with glasses\n",
    "glasses = [\n",
    "    (10, 19), (30, 32), (37, 38), (50, 59), (63, 64),\n",
    "    (69, 69), (120, 121), (124, 129), (130, 139), (160, 161),\n",
    "    (164, 169), (180, 182), (185, 185), (189, 189), (190, 192),\n",
    "    (194, 194), (196, 199), (260, 269), (270, 279), (300, 309),\n",
    "    (330, 339), (358, 359), (360, 369)\n",
    "]\n",
    "\n",
    "def tag_positive_examples(segments):\n",
    "    # create a new y array of target size initialized with zeros\n",
    "    y = np.zeros(faces.target.shape[0])\n",
    "    # put 1 in the specified segments\n",
    "    for (start, end) in segments:\n",
    "        y[start:end + 1] = 1\n",
    "    return y\n",
    "\n",
    "have_glasses = tag_positive_examples(glasses)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        faces.data, have_glasses, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_2 = SVC(kernel='linear')\n",
    "evaluate_cross_validation(svc_2, X_train, y_train, 5)\n",
    "train_and_evaluate(svc_2, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost perfect! Now, let's hold out 10 images (all from the same person, sometimes with glasses and sometimes without glasses).  We'll separate the subject with indexes from 30 to 39.\n",
    "\n",
    "The aim is to see if the SVM can pick up glasses-related features (and not be dependent on having seen a particular face with or without glasses in the training set). We'll train and evaluate in the rest of the 390 instances. After that, we'll evaluate again over the separated 10 instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = faces.data[30:40]\n",
    "y_test = target_glasses[30:40]\n",
    "\n",
    "print(y_test.shape[0])\n",
    "\n",
    "select = np.ones(target_glasses.shape[0]) # <-- here we set an array all set to 1\n",
    "select[30:40] = 0 #<-- here we drop any indices already assigned to testing \n",
    "X_train = faces.data[select == 1] #<-- we use the above array as a mask to get the same elements out of each of the following..\n",
    "X_images = faces.images[select == 1]\n",
    "y_train = have_glasses[select == 1]\n",
    "\n",
    "#rjm49 - train up and run linear SVM classifier\n",
    "svcs = []\n",
    "for k in (\"linear\", \"poly\", \"rbf\"):\n",
    "    this_svc = SVC(kernel=k)\n",
    "    print(\"Kernel is\",k)\n",
    "    train_and_evaluate(this_svc, X_train, X_test, y_train, y_test)\n",
    "    y_pred = this_svc.predict(X_test)\n",
    "    svcs.append((k,this_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show our test-set faces, and their predicted category. Picture number eight is incorrectly classified as no-glasses (probably because his eyes are closed!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_faces = [np.reshape(a, (64, 64)) for a in X_test]\n",
    "print(\"True labels\")\n",
    "print_faces(eval_faces, y_test, top_n=10)\n",
    "print(\"Pred labels\")\n",
    "print_faces(eval_faces, y_pred, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Kernel functions to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "Xhard, yhard = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=2, class_sep=0.5)\n",
    "\n",
    "datasets = [(X,y),\n",
    "            (Xhard,yhard),\n",
    "            make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            ]\n",
    "classifiers = svcs\n",
    "\n",
    "h = .02\n",
    "i = 1\n",
    "plt.figure(figsize=(12,3*len(datasets)))\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "#     ax = plt.gca()\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "    \n",
    "        # iterate over classifiers\n",
    "    for name, clf in classifiers:\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "#         ax = plt.gca()\n",
    "        clf.fit(X_train, y_train)\n",
    "#         score = clf.score(X_test, y_test)\n",
    "        y_hats = clf.predict(X_test)\n",
    "        score = f1_score(y_test, y_hats, average=\"weighted\")\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name +\" \"+ type(clf).__name__)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('acc={:.2f}'.format(score).lstrip('0')),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
