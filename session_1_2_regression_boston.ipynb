{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1.2 Regression (Multivariate, Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "To demonstrate multivariate regression in scikit-learn, we will apply it to a (very) simple and well-know problem: trying to predict the price of a house given some of its characteristics. As the dataset, we will use the 1978 Boston house price dataset (find the dataset description and attributes [here](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "print(type(boston))\n",
    "\n",
    "print ('Boston dataset shape:{}'.format(boston.data.shape))\n",
    "print ('Boston target shape:{}'.format(boston.target.shape))\n",
    "print (boston.feature_names)\n",
    "\n",
    "manual_split = True\n",
    "if manual_split:\n",
    "    numpy.random.seed(666)\n",
    "    test_split = 0.2\n",
    "    test_n = int(test_split*len(boston.data))\n",
    "    test_indices = numpy.random.randint(0,len(boston.data), size=test_n)\n",
    "    train_indices = sorted(set(range(len(boston.data))) - set(test_indices))\n",
    "\n",
    "    X_train_boston_raw=boston.data[train_indices]\n",
    "    X_test_boston_raw=boston.data[test_indices]\n",
    "    y_train_boston=boston.target[train_indices]\n",
    "    y_test_boston=boston.target[test_indices]\n",
    "else:\n",
    "    X_train_boston_raw, X_test_boston_raw, y_train_boston, y_test_boston = train_test_split(boston.data, boston.target, test_size=0.2, random_state=666)\n",
    "\n",
    "\n",
    "numpy.set_printoptions(precision=4)\n",
    "print(\"features:\\n\", X_train_boston_raw[0:3,:])\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "\n",
    "#Create our scaled train and test datasets\n",
    "X_train_boston = x_scaler.fit_transform(X_train_boston_raw) # preprocessing.scale(X_train_boston_raw) # shortcut to preprocessing.StandardScaler()\n",
    "X_test_boston = x_scaler.transform(X_test_boston_raw)\n",
    "\n",
    "print(\"Scaled features:\\n\", X_train_boston[0:3,:])\n",
    "print(\"prices:\\n\", y_train_boston[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using n-fold cross-validation\n",
    "\n",
    "Previously we've trained using a dataset split into train and test subsets.  Another way to split your data is to use cross validation.\n",
    "\n",
    "One of the main advantages of cross-validation is reducing the variance of the evaluation measures.  When you split the data manually, you will find that for each different split, your algorithm's performance will vary.  How do you know what is the right score?\n",
    "\n",
    "Evaluation within machine learning generally assumes that the distribution of classes on your training and testing sets are similar. If not, you may get results that are not a truthful measure of the classifier's performance. Cross-validation lets us mitigate this: we are averaging on k different models built on k different datasets, so we are reducing variance and probably producing more realistic performance scores for our models.\n",
    "\n",
    "Another benefit of cross-validation is that it allows us to make good use of the data we have available - each example acts as both a training datapoint and as a validation datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(_clf, X_train, y_train, n_folds):\n",
    "    _clf.fit(X_train, y_train)\n",
    "    print ('Score on training set: {:.2f}'.format(_clf.score(X_train, y_train)))\n",
    "    #create a k-fold cross validation iterator of k=5 folds\n",
    "    data =X_train.shape[0]\n",
    "    cv = sklearn.model_selection.KFold(n_splits= n_folds, shuffle=True, random_state=42)\n",
    "    scores = sklearn.model_selection.cross_val_score(_clf, X_train, y_train, cv=cv)\n",
    "    print ('Average score using {}-fold crossvalidation:{:.2f}'.format(n_folds,np.mean(scores)))\n",
    "    return _clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we used accuracy, the proportion of correctly classified test-instances, to summarise our methodâ€™s performance.\n",
    "\n",
    "For regression, accuracy is a bad idea: we are predicting real values, so it's almost impossible to exactly predict the true value.\n",
    "\n",
    "Instead, the default score function in scikit-learn is the coefficient of determination (or $R^2$ score), which measures the proportion of outcome variation explained by the model. $R^2 \\in [0,1]$, and reaches 1 when the model perfectly predicts all the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#Use a Stochastic Gradient Descent Regressor - this is a general purpose linear regressor good for large datasets\n",
    "clf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None, random_state=42, max_iter=10e5, tol=1e-4)\n",
    "train_and_evaluate(clf_sgd, X_train_boston, y_train_boston, 5)\n",
    "\n",
    "#print the hyperplane coefficients and their sum-of-squares\n",
    "\n",
    "print(clf_sgd.coef_)\n",
    "print(np.sum(np.square(clf_sgd.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_sgd.score(X_test_boston, y_test_boston))\n",
    "y_hats = clf_sgd.predict(X_test_boston)\n",
    "for y,yh in zip(y_test_boston, y_hats):\n",
    "    print(y,yh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a correlation matrix to help us pick out the most relevant factors.  We want those with the biggest (negative or positive) correlation with median value, MEDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = numpy.corrcoef(X_train_boston.T, y_train_boston)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(correlation_matrix)\n",
    "\n",
    "# # We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(boston.data.T)+1))\n",
    "ax.set_yticks(np.arange(len(boston.data.T)+1))\n",
    "# # ... and label them with the respective list entries\n",
    "ax.set_xticklabels(list(boston.feature_names)+[\"MEDV\"])\n",
    "ax.set_yticklabels(list(boston.feature_names)+[\"MEDV\"])\n",
    "\n",
    "\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(boston.feature_names)+1):\n",
    "    for j in range(len(boston.feature_names)+1):\n",
    "        text = ax.text(j, i, round(correlation_matrix[i, j],1),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Correlation matrix of Boston house price variables\")\n",
    "# fig.tight_layout()\n",
    "fig.set_size_inches(6,6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like LSTAT and RM are most relevant.  Sklearn lets us automatically extra the K best features for explaining variance in the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import *\n",
    "k=5\n",
    "\n",
    "fs=SelectKBest(score_func=f_regression,k=k)\n",
    "X_new=fs.fit_transform(X_train_boston,y_train_boston)\n",
    "print (zip(fs.get_support(),boston.feature_names))\n",
    "\n",
    "x_min, x_max = X_new[:,0].min() - .5, X_new[:, 0].max() + .5\n",
    "y_min, y_max = y_train_boston.min() - .5, y_train_boston.max() + .5\n",
    "#fig=plt.figure()\n",
    "\n",
    "# Two subplots, unpack the axes array immediately\n",
    "fig, axes = plt.subplots(1,k)\n",
    "# plt.tight_layout()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.1)\n",
    "fig.set_size_inches(12,6)\n",
    "fig.tight_layout()\n",
    "\n",
    "cols = fs.get_support(indices=True)\n",
    "print(cols)\n",
    "\n",
    "for i in range(k):\n",
    "    axes[i].set_aspect('auto')\n",
    "#     axes[i].set_aspect('equal')\n",
    "    axes[i].set_title('Feature ' + boston.feature_names[cols[i]])\n",
    "    axes[i].set_xlabel('Feature')\n",
    "    axes[i].set_ylabel('Median house value')\n",
    "    axes[i].set_xlim(x_min, x_max)\n",
    "    axes[i].set_ylim(y_min, y_max)\n",
    "    sca(axes[i])\n",
    "    plt.scatter(X_new[:,i],y_train_boston, alpha=0.2)\n",
    "    \n",
    "X_test_new = X_test_boston[:,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default score function in scikit-learn is the _coefficient of determination_ (or $R^2$ score), which measures the proportion of outcome variation explained by the model. $R^2 \\in [0,1]$, and reaches 1 when the model perfectly predicts all the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#Use a Stochastic Gradient Descent Regressor - this is a general purpose linear regressor good for large datasets\n",
    "train_and_evaluate(clf_sgd, X_new, y_train_boston, 5)\n",
    "\n",
    "#print the hyperplane coefficients and their sum-of-squares\n",
    "print(clf_sgd.coef_)\n",
    "print(np.sum(np.square(clf_sgd.coef_)))\n",
    "\n",
    "print(X_test_new.shape)\n",
    "clf_sgd.fit(X_new, y_train_boston)\n",
    "print(clf_sgd.score(X_test_new, y_test_boston))\n",
    "y_hats = clf_sgd.predict(X_test_new)\n",
    "for y,yh in zip(y_test_boston, y_hats):\n",
    "    print(y,yh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "Use a _non-linear_ regressor such as sklearn's SVR, and cross validate it on the Boston data.  Is it better?  If so, why might this be?  What about on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "clf_sgd = SVR()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- We tried out multivariate regression on the Boston house price dataset, using k-fold cross validation to test our estimators\n",
    "- We tried some feature selection using a correlation matrix and SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
