{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambridge ML Commando Course\n",
    "## Session 2.1 - Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "from sklearn import preprocessing\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hours of study...\n",
    "A super-simple example to start us off.  We have a dataset that matches hours of study to the observation of whether a student passed a test.  The test result is dichotomous (0 for fail, 1 for pass).  How do we fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#Hours, pass\n",
    "study_data = [(0.50, 0),\n",
    "(0.75, 0),\n",
    "(1.00, 0),\n",
    "(1.25, 0),\n",
    "(1.50, 0),\n",
    "(1.75, 0),\n",
    "(1.75, 1),\n",
    "(2.00, 0),\n",
    "(2.25, 1),\n",
    "(2.50, 0),\n",
    "(2.75, 1),\n",
    "(3.00, 0),\n",
    "(3.25, 1),\n",
    "(3.50, 0),\n",
    "(4.00, 1),\n",
    "(4.25, 1),\n",
    "(4.50, 1),\n",
    "(4.75, 1),\n",
    "(5.00, 1),\n",
    "(5.50, 1) ]\n",
    "\n",
    "X = numpy.array(study_data)[:,0].reshape(-1,1)\n",
    "y = numpy.array(study_data)[:,1].ravel()\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "print(\"Accuracy (training) is\", lr.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main dataset for this book\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.target)\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "generate_synthetic_dataset = False\n",
    "\n",
    "if generate_synthetic_dataset == True:\n",
    "    dataset=\"synthetic data\"\n",
    "    n_f = 2\n",
    "    n_c = 2\n",
    "    X,y = sklearn.datasets.make_classification(n_features=n_f, n_informative=2, n_redundant=0, n_classes=n_c, n_clusters_per_class=2, random_state=42)\n",
    "    y_names = [\"Class {}\".format(ix) for ix in range(n_c)]\n",
    "    X_names = [\"Feat {}\".format(ix) for ix in range(n_f)]\n",
    "else:\n",
    "    for dataset in [\"iris\"]: # we can iterate across them all with [\"iris\",\"wine\",\"breast_cancer\"]:\n",
    "        if dataset == \"iris\":\n",
    "            remove_a_species = True # we can use this to make the Iris dataset binary\n",
    "            petals_only = True # we can use this to make the Iris data 2D\n",
    "            iris = datasets.load_iris()\n",
    "#             print(iris.DESCR) #this gives a fairly long description of the dataset\n",
    "            X_names = iris.feature_names\n",
    "            y_names = iris.target_names\n",
    "            \n",
    "            if remove_a_species:\n",
    "                remove_which = \"setosa\"\n",
    "                class_to_exclude = list(y_names).index(remove_which)\n",
    "                y_names = np.delete(y_names,class_to_exclude)\n",
    "                print(y_names)\n",
    "                \n",
    "                indices_2class = (iris.target != class_to_exclude) #[False for non-class_to_exclude else True]\n",
    "                X = iris.data[indices_2class]\n",
    "                y = iris.target[indices_2class]\n",
    "            else:\n",
    "                X = iris.data\n",
    "                y = iris.target\n",
    "\n",
    "            if petals_only:\n",
    "                # sepal values are 0:2\n",
    "                X = X[:, 2:4]\n",
    "                X_names = X_names[2:4]\n",
    "                \n",
    "        elif dataset==\"wine\":\n",
    "            wine_data = datasets.load_wine()\n",
    "            X = wine_data.data\n",
    "            X_names = wine_data.feature_names\n",
    "            y = wine_data.target\n",
    "            y_names = wine_data.target_names\n",
    "        elif dataset==\"breast_cancer\":\n",
    "            bc_data = datasets.load_breast_cancer()\n",
    "            X = bc_data.data\n",
    "            X_names = bc_data.feature_names\n",
    "            y = bc_data.target\n",
    "            y_names = bc_data.target_names\n",
    "            \n",
    "        print(X_names)\n",
    "        print(y_names)\n",
    "        print(X.shape)\n",
    "            \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=666)\n",
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "plt.set_cmap(\"Paired\")\n",
    "ax = plt.gca()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "if X_train.shape[1] >2:\n",
    "    reduce_to_2d = PCA(n_components=2) # flatten the dataset down to 2D so we can actually plot it\n",
    "    X_tr_vis = reduce_to_2d.fit_transform(X_train)\n",
    "    X_tt_vis = reduce_to_2d.transform(X_test)\n",
    "    plt.xlabel(\"Principal Comp 1\")\n",
    "    plt.ylabel(\"Principal Comp 2\")\n",
    "else:\n",
    "    reduce_to_2d = None\n",
    "    print(X_train.shape)\n",
    "    X_tr_vis = X_train\n",
    "    X_tt_vis = X_test\n",
    "    plt.xlabel(X_names[0])\n",
    "    plt.ylabel(X_names[1])\n",
    "\n",
    "ax.scatter(X_tr_vis[:,0], X_tr_vis[:,1], c=y_train, alpha=0.7)#, cmap=plt.cm.Paired)\n",
    "ax.scatter(X_tt_vis[:,0], X_tt_vis[:,1], marker=\"s\", s=50, c=y_test, edgecolor=\"k\")#, cmap=plt.cm.Paired) #it's sneaky to look at your test data!\n",
    "plt.title(dataset)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(estr, X, y, K):\n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    print(\"underway...\")\n",
    "    k_cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(estr, X, y, cv=k_cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "def plot_matrix(mx, title=\"Some Matrix\", labels = None):\n",
    "    plt.set_cmap(\"jet\")\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    im = ax.imshow(minmax_scale(mx))\n",
    "\n",
    "    # # We want to show all ticks...\n",
    "    if labels is not None:\n",
    "        ax.set_xticks(np.arange(len(labels)))\n",
    "        ax.set_yticks(np.arange(len(labels)))\n",
    "\n",
    "        # # ... and label them with the respective list entries\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_yticklabels(labels)\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "#     Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(mx)):\n",
    "        for j in range(len(mx)):\n",
    "            text = ax.text(j, i, round(mx[i, j],1),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    # fig.tight_layout()\n",
    "    fig.set_size_inches(6,6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn import svm\n",
    "reg = LogisticRegression(solver=\"lbfgs\", multi_class=\"ovr\")\n",
    "# reg = svm.SVC(kernel=\"rbf\", probability=True)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) # 2 decimal places, suppress scientific notation for legibility\n",
    "\n",
    "evaluate_cross_validation(reg, X_train, y_train, 5)\n",
    "y_hats_tr = reg.predict(X_train)\n",
    "y_hats_proba = reg.predict_proba(X_train)\n",
    "print(\"y yh[class probabilities]:\")\n",
    "for y,yh,yhp in zip(y_train, y_hats_tr, y_hats_proba):\n",
    "    print(\"True v pred: {},{} ~\".format(y, yh), \"Class Probs:\", yhp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_tr_vis[:, 0].min() - .5, X_tr_vis[:, 0].max() + .5\n",
    "y_min, y_max = X_tr_vis[:, 1].min() - .5, X_tr_vis[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "if reduce_to_2d is not None:\n",
    "    Z = reg.predict(reduce_to_2d.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "else:\n",
    "    Z = reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(6, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot the training and test points\n",
    "plt.scatter(X_tr_vis[:,0], X_tr_vis[:,1], c=y_train, marker=\"o\", s=20, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.scatter(X_tt_vis[:,0], X_tt_vis[:,1], c=y_test, marker=\"s\", s=50, edgecolors='w', alpha=0.7, cmap=plt.cm.Paired)\n",
    "\n",
    "if reduce_to_2d is None:\n",
    "    plt.xlabel(X_names[0])\n",
    "    plt.ylabel(X_names[1])\n",
    "else:\n",
    "    plt.xlabel('Prin Comp 1')\n",
    "    plt.ylabel('Prin Comp 2')\n",
    "    \n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print(\"TRAINING:\")\n",
    "y_hats_tr = reg.predict(X_train)\n",
    "print(\"Acc:\", accuracy_score(y_train, y_hats_tr))\n",
    "# print(\"p/r/F/supp:\",sklearn.metrics.precision_recall_fscore_support(y_train, y_hats_tr, average='micro'))\n",
    "print (\"Classification Report:\")\n",
    "print (classification_report(y_train, y_hats_tr, target_names=y_names))\n",
    "print(\"\\nConfusion Mx (training data):\")\n",
    "plt.set_cmap(\"jet\")\n",
    "mx = confusion_matrix(y_train, y_hats_tr)\n",
    "plot_matrix(mx, title=\"Confusion (Train)\", labels = y_names)\n",
    "\n",
    "print(\"\\nTEST\")\n",
    "y_hats = reg.predict(X_test)\n",
    "print(\"Acc:\", accuracy_score(y_test, y_hats))\n",
    "# print(\"p/r/F/supp:\",sklearn.metrics.precision_recall_fscore_support(y_test, y_hats, average='micro'))\n",
    "print (\"Classification Report:\")\n",
    "print (classification_report(y_test, y_hats, target_names=y_names))\n",
    "print(\"\\nConfusion Mx (test data):\")\n",
    "mx = confusion_matrix(y_test, y_hats)\n",
    "plot_matrix(mx, title=\"Confusion (Test)\", labels = y_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial classification\n",
    "Now reintroduce the setosa class, and try running the same routines again with a non-binary dataset.\n",
    "\n",
    "## Summary\n",
    "- We loaded the Iris dataset and manually split it into training and test sets\n",
    "- We used Logistic Regression to classify the test data\n",
    "- We checked out the decision boundary by plotting the class partitions\n",
    "- We got P/R/F and Confusion Matrices on train and test\n",
    "- We repeated with all 3 classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
