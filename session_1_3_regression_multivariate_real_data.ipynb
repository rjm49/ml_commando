{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1.2 Regression (Multivariate, real data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# print ('Python version:', platform.python_version())\n",
    "# print ('IPython version:', IPython.__version__)\n",
    "# print ('numpy version:', np.__version__)\n",
    "# print ('scikit-learn version:', sklearn.__version__)\n",
    "# print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your multivariate data\n",
    "\n",
    "To demonstrate multivariate regression in scikit-learn, we will apply it to a (very) simple and well-know problem: trying to predict the price of a house given some of its characteristics. As the dataset, we will use the 1978 Boston house price dataset (find the dataset description and attributes [here](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)).\n",
    "\n",
    "This notebook is set up so that you can load some **other datasets** (Cheddar Cheese Tastiness, California House Prices) and also **generate your own** (set gen_data = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, load_boston, fetch_california_housing\n",
    "import pandas as pd\n",
    "gen_data = False\n",
    "clean_up = True\n",
    "# data_load = \"california\"\n",
    "if not gen_data:\n",
    "#     for data_load in [\"cheese\", \"boston\", \"california\"]:\n",
    "    for data_load in [\"boston\",]:\n",
    "        if data_load == \"cheese\":\n",
    "            print(\"\\nCHEESE\")\n",
    "            data = pd.read_csv(\"./data/cheddar-cheese.csv\")\n",
    "            print(data)\n",
    "            X_feature_names = data.columns[1:4]\n",
    "            y_feature_name = data.columns[-1]\n",
    "            data = data.values\n",
    "            X = data[:, 1:4] # all rows (:), columns 1-3 (1:3)\n",
    "            y = data[:, -1] # just the last column (col 4)\n",
    "\n",
    "        elif data_load == \"boston\":\n",
    "            print(\"\\nBOSTON\")\n",
    "            data_bunch = load_boston() #fetch_california_housing() #load_diabetes()\n",
    "            print(data_bunch.DESCR)\n",
    "            X = data_bunch.data \n",
    "            y = data_bunch.target\n",
    "            X_feature_names = data_bunch.feature_names\n",
    "            y_feature_name = data_bunch.target_names if \"target_names\" in data_bunch else \"target\"\n",
    "\n",
    "            if clean_up:\n",
    "                X = np.delete(X, -2, axis=1) # remove the dubious \"B\" feature\n",
    "                X_feature_names = np.delete(X_feature_names, -2)\n",
    "\n",
    "                sub_50k_indices = (y < 50) # remove clipped values at the max price ($50k)\n",
    "                y = y[sub_50k_indices]\n",
    "                X = X[sub_50k_indices, :]\n",
    "\n",
    "        elif data_load == \"california\":\n",
    "            print(\"\\nCALI\")\n",
    "            data_bunch = fetch_california_housing()\n",
    "            print(data_bunch.DESCR)\n",
    "            X = data_bunch.data \n",
    "            y = data_bunch.target\n",
    "            X_feature_names = data_bunch.feature_names\n",
    "            y_feature_name = data_bunch.target_names if \"target_names\" in data_bunch else \"target\"\n",
    "\n",
    "            if clean_up:\n",
    "                sub_50k_indices = (y < 5) # remove clipped values at the max price ($50k)\n",
    "                y = y[sub_50k_indices]\n",
    "                X = X[sub_50k_indices, :]\n",
    "else:\n",
    "    print(gen_data)\n",
    "    n_f = 10\n",
    "    n_inf = n_f//3 # this is an integer division ... could also do = int(n_f/10)\n",
    "    X, y = make_regression(n_samples=100, n_features=n_f, n_informative=n_inf, noise=25, random_state=666)\n",
    "    X_feature_names = [\"Feat.{}\".format(f) for f in range(n_f)]\n",
    "    y_feature_name = \"Target\"\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_feature_names, y_feature_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the datasets\n",
    "This time we'll use sklearn's own utility function to do the dataset partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.2, random_state=666)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(\"features:\\n\", X_train_raw[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale your multivariate data\n",
    "It is important for multivariate data to scale the dataset, in particular when features are measured in wildly different units, since otherwise those with large values will tend to eclipse the smaller ones and skew the result. To make sure data is all with the same ranges, we use a scaler.\n",
    "\n",
    "Sklearn's StandardScaler transforms all ranges to have a mean of 0 and a std. dev of 1.  Once a scaler has been fit, it can be reused to apply the same scaling again.\n",
    "\n",
    "Don't forget to scale the test X data as well as the training X data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for do_scale in [False, True]:\n",
    "    # Create our scaled train and test datasets\n",
    "    if do_scale:\n",
    "        print(\"Check the scaled values for mean, std\")\n",
    "        x_scaler = StandardScaler()\n",
    "        X_train = x_scaler.fit_transform(X_train_raw)\n",
    "        X_test  = x_scaler.transform(X_test_raw) # we don't re-fit on test data (imagine we've never seen it before...)\n",
    "    else:\n",
    "        print(\"Check the unscaled values for mean, std\")\n",
    "        X_train = X_train_raw\n",
    "        X_test  = X_test_raw\n",
    "    \n",
    "    for s,d in [(\"train\",X_train), (\"test\",X_test)]:\n",
    "        print(\"Feature means ({}):\".format(s), numpy.mean(d, axis=0))\n",
    "        print(\"Feature stdevs ({}):\".format(s), numpy.std(d, axis=0))\n",
    "        print(\"---\")\n",
    "        \n",
    "    y_train = y_train_raw\n",
    "    y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using n-fold cross-validation\n",
    "\n",
    "Previously we've trained using a dataset split into train and test subsets.  Another way to split your data is to use cross validation.\n",
    "\n",
    "One of the main advantages of cross-validation is reducing the variance of the evaluation measures.  When you split the data manually, you will find that for each different split, your algorithm's performance will vary.  How do you know what is the right score?\n",
    "\n",
    "Evaluation within machine learning generally assumes that the distribution of classes on your training and testing sets are similar. If not, you may get results that are not a truthful measure of the classifier's performance. Cross-validation lets us mitigate this: we are averaging on k different models built on k different datasets, so we are reducing variance and probably producing more realistic performance scores for our models.\n",
    "\n",
    "Another benefit of cross-validation is that it allows us to make good use of the data we have available - each example acts as both a training datapoint and as a validation datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def train_and_evaluate(_reg, _X, _y, n_folds=0, refit=False):\n",
    "    # Estimator objects can be fitted or unfitted.  If the user wants to force a retraining, then we do not need to check the status of the estimator\n",
    "    if refit==True:\n",
    "        do_train=True\n",
    "    else:\n",
    "        try:\n",
    "            check_is_fitted(_reg)\n",
    "            print(\"Estimator is already fitted: will not train it again!\")\n",
    "            do_train=False\n",
    "        except:\n",
    "            do_train=True\n",
    "\n",
    "    if do_train:\n",
    "        _reg.fit(_X, _y)\n",
    "        print ('Score on training set: {:.2f}'.format(_reg.score(_X, _y)))\n",
    "\n",
    "    if n_folds >= 2: # we need a couple of folds to do cross validation\n",
    "        cv = sklearn.model_selection.KFold(n_splits= n_folds, shuffle=True, random_state=666)\n",
    "        scores = sklearn.model_selection.cross_val_score(_reg, _X, _y, cv=cv, scoring=\"explained_variance\")\n",
    "        av_score = np.mean(scores)\n",
    "        print ('Average score using {}-fold crossvalidation:{:.2f}'.format(n_folds,av_score))\n",
    "    else: #otherwise just do a normal scoring\n",
    "        av_score = sklearn.metrics.explained_variance_score(_reg.predict(_X),_y)\n",
    "        print(\"Plain (non-CV) score on X vs y:{:.2f}\".format(av_score))\n",
    "    return _reg, av_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "est = LinearRegression()\n",
    "train_and_evaluate(est, X_train, y_train, 5) # this will train the estimator and do cross validation\n",
    "train_and_evaluate(est, X_test, y_test, 0) # this will NOT train a trained estimator, and with n_folds=0 it will also not cross validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection with a correlation matrix\n",
    "Create a correlation matrix to help us pick out the most relevant factors.  We want those with the biggest (negative or positive) correlation with median value, MEDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = numpy.corrcoef(X_train.T, y_train)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(correlation_matrix)\n",
    "\n",
    "print(X_feature_names)\n",
    "feat_names = [str(x) for x in X_feature_names] + [y_feature_name] # concatenate these two lists\n",
    "\n",
    "mx_dim = X_train.shape[1]+1\n",
    "# # We want to show all ticks...\n",
    "ax.set_xticks(np.arange(mx_dim))\n",
    "ax.set_yticks(np.arange(mx_dim))\n",
    "# # ... and label them with the respective list entries\n",
    "ax.set_xticklabels(feat_names)\n",
    "ax.set_yticklabels(feat_names)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(mx_dim):\n",
    "    for j in range(mx_dim):\n",
    "        text = ax.text(j, i, round(correlation_matrix[i, j],1),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Correlation matrix\")\n",
    "# fig.tight_layout()\n",
    "fig.set_size_inches(6,6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic feature selection\n",
    "Sklearn lets us automatically test (F-test!) and select the K best features for explaining variance in the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import * # bad python, don't do this :)\n",
    "k=5\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "fs=SelectKBest(score_func=f_regression,k=k)\n",
    "X_new=fs.fit_transform(X_train,y_train)\n",
    "X_test_new = fs.transform(X_test)\n",
    "\n",
    "# X_new = X_train #uncomment to keep all the features\n",
    "# X_test_new = X_test\n",
    "\n",
    "fig, axes = plt.subplots(1, X.shape[1])\n",
    "fig.set_size_inches(12,6)\n",
    "for i in range(X.shape[1]):\n",
    "    axes[i].set_aspect('auto')\n",
    "    axes[i].scatter(X_train[:,i], y_train, alpha=0.5)\n",
    "    axes[i].set_title(X_feature_names[i])\n",
    "plt.show()\n",
    "columns_to_keep = np.argsort(fs.pvalues_)[0:k]\n",
    "print(columns_to_keep, fs.pvalues_[columns_to_keep])\n",
    "\n",
    "# Two subplots, unpack the axes array immediately\n",
    "if k >1:\n",
    "    fig, axes = plt.subplots(1,k)\n",
    "else:\n",
    "    fig = plt.gcf()\n",
    "    axes = [plt.gca()]\n",
    "    \n",
    "fig.set_size_inches(12,6)\n",
    "# fig, axes = plt.subplots(1, X.shape[1])\n",
    "# fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.1)\n",
    "fig.tight_layout()\n",
    "sneaky_peek = True\n",
    "if k>0:\n",
    "    for i, f_idx in enumerate(columns_to_keep):\n",
    "        axes[i].set_aspect('auto')\n",
    "        axes[i].set_title('F.' + str(f_idx) + \" \" + X_feature_names[f_idx])\n",
    "        axes[i].set_xlabel('Feature')\n",
    "        axes[i].set_ylabel(y_feature_name)\n",
    "        fig.sca(axes[i])\n",
    "        axes[i].scatter(X_train[:,f_idx],y_train, alpha=0.5, label=\"train\")\n",
    "        if sneaky_peek:\n",
    "            axes[i].scatter(X_test[:,f_idx],y_test, alpha=0.5, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for new non-linear features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# print(\"\\nSVR RBF (Top 5)\")\n",
    "# svr_rbf = svm.SVR(kernel=\"rbf\")#, degree=2)#, gamma=\"auto\")\n",
    "# train_and_evaluate(svr_rbf, X_new, y_train, 5)\n",
    "\n",
    "def poly_features_param_search(_est, _X,_y, _X_test, _y_test):\n",
    "    best = (None, None, -math.inf, -math.inf)\n",
    "    for deg in [1,2,3,4]:\n",
    "        for inter_only in [False, True]:\n",
    "            polyfeat = PolynomialFeatures(degree=deg, interaction_only=inter_only)\n",
    "            X_f = polyfeat.fit_transform(_X)\n",
    "            X_f_test = polyfeat.transform(_X_test)\n",
    "            _est, sc_tr = train_and_evaluate(_est, X_f, _y, 5, refit=True)\n",
    "            if sc_tr > best[2]:\n",
    "                print(\"New best on training\")\n",
    "                print(\"\\t\", end=\"\")\n",
    "                _est, sc_tt = train_and_evaluate(_est, X_f_test, _y_test, 0, refit=False)\n",
    "                best = (deg, inter_only, sc_tr, sc_tt)\n",
    "    print(best)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = LinearRegression()\n",
    "poly_features_param_search(est, X_train, y_train, X_test, y_test)\n",
    "print(\"~~~~\\n\")\n",
    "ptx = PolynomialFeatures(2, interaction_only=True)\n",
    "X_poly = ptx.fit_transform(X_train)\n",
    "X_test_poly = ptx.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying some non-linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "Use a _non-linear_ regressor such as sklearn's SVR, and cross validate it on the Boston data.  Is it better?  If so, why might this be?  What about on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = \"All features\"\n",
    "top_feats = \"Top features only\"\n",
    "\n",
    "dataset_lookup = {}\n",
    "dataset_lookup[all_feats] = (X_train, X_test)\n",
    "dataset_lookup[top_feats] = (X_poly, X_test_poly)\n",
    "dataset_lookup[\"target\"] = (y_train, y_test)\n",
    "\n",
    "from sklearn import svm, neighbors, tree\n",
    "svr_linear = svm.SVR(kernel=\"linear\")\n",
    "svr_rbf = svm.SVR(kernel=\"rbf\")\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "dtr = tree.DecisionTreeRegressor()\n",
    "est_list = [svr_linear, svr_rbf, knn, dtr]\n",
    "\n",
    "for est in est_list:\n",
    "    for feats_name in [all_feats, top_feats]:\n",
    "        X_tr, X_tt = dataset_lookup[feats_name]\n",
    "        y_tr, y_tt = dataset_lookup[\"target\"]\n",
    "        print(X_tr.shape, X_tt.shape)\n",
    "        print(y_tr.shape, y_tt.shape)\n",
    "        print(\"{}, {}\".format(est.__class__.__name__, feats_name))\n",
    "#         for k,v in est.get_params().items():\n",
    "#             print(\"\\t{}:{}\".format(k,v))\n",
    "        train_and_evaluate(est, X_tr, y_tr, 5, refit=True)\n",
    "        train_and_evaluate(est, X_tt, y_tt, 0, refit=False)\n",
    "        print()\n",
    "    print(\"~~~\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- We tried out multivariate regression on the Boston house price dataset, using k-fold cross validation to test our estimators\n",
    "- We tried some feature selection using a correlation matrix and SelectKBest\n",
    "- We used the PolynomialFeatures transformer to construct new polynomial feature sets that can model curves and created a simple parameter search routine to optimise the polynomial degree, and interaction features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
