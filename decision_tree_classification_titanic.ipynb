{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambridge ML Commando Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic: Survival Prediction with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The problem we would like to solve is to determine if a Titanic's passenger would have survived, given age, passenger class, and sex. We will work with the Titanic dataset that can be downloaded from http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import pyparsing\n",
    "\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)\n",
    "#print ('pydot version:', pydotplus.__version__)\n",
    "print ('pyparsing version:', pyparsing.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we should first load the dataset. The list of attributes it includes is: Ordinal, Class, Survived (0=no, 1=yes), Name, Age, Port of Embarkation, Home/Destination, Room, Ticket, Boat, and Sex. We will start by loading the dataset into a numpy array.. We assume it is located in the data/titanic.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"data/titanic.csv\")\n",
    "print(df[0:10])\n",
    "\n",
    "df = df[[\"pclass\",\"age\",\"sex\",\"survived\"]]\n",
    "\n",
    "mean_age = np.mean(df[\"age\"])\n",
    "df[\"age\"] = df[\"age\"].fillna(mean_age)\n",
    "\n",
    "gender_enc = LabelEncoder()\n",
    "df[\"sex\"] = gender_enc.fit_transform(df[\"sex\"])\n",
    "\n",
    "class_enc = LabelEncoder()\n",
    "df[\"pclass\"] = class_enc.fit_transform(df[\"pclass\"])\n",
    "print(\"\\nCLEANED----\\n\",df[0:10])\n",
    "\n",
    "titanic_X = df[[\"pclass\",\"age\",\"sex\"]]\n",
    "titanic_y = df[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import csv\n",
    "# # with open('data/titanic.csv') as csvfile:\n",
    "# #     titanic_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "# #     # Header contains feature names\n",
    "# #     row = next(titanic_reader)\n",
    "# #     feature_names = np.array(row)\n",
    "# #     # Load dataset, and target classes\n",
    "# #     titanic_X, titanic_y = [], []\n",
    "# #     for row in titanic_reader:  \n",
    "# #         titanic_X.append(row)\n",
    "# #         titanic_y.append(row[2]) # The target value is \"survived\"\n",
    "# #     titanic_X = np.array(titanic_X)\n",
    "# #     titanic_y = np.array(titanic_y)\n",
    "\n",
    "# # Let's inspect how data looks. \n",
    "\n",
    "# print (feature_names,\"\\n\", titanic_X[0:10],\"\\n\", titanic_y[0:10],\"\\n\")\n",
    "# print (titanic_y[0:10])\n",
    "\n",
    "# # We will keep only class (1st,2nd,3rd), age (float), and sex (male, female) for our study. \n",
    "\n",
    "# # we keep the class, the age and the sex\n",
    "# titanic_X = titanic_X[:, [1, 4, 10]]\n",
    "# feature_names = feature_names[[1, 4, 10]]\n",
    "# print(feature_names)\n",
    "# print(titanic_X[12], titanic_y[12])\n",
    "\n",
    "# #We have some problems with missing values ('NA') for the 'age' feature. To avoid this, we will use the mean value whenever we do not have data available.\n",
    "\n",
    "# ages = titanic_X[:, 1]\n",
    "# mean_age = np.mean(titanic_X[ages != 'NA', 1].astype(np.float))\n",
    "# titanic_X[titanic_X[:, 1] == 'NA', 1] = mean_age\n",
    "# print(feature_names)\n",
    "# print(titanic_X[12], titanic_y[12])\n",
    "\n",
    "# ##Now, class and sex are categorical classes, but most scikit-learn classifiers (in particular the Decision Trees we plan to use), expect real-valued attributes. We can easily convert sex  to a binary value (0=female,1=male). We will use the LabelEncoder class from scikit-learn:\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# enc = LabelEncoder()\n",
    "# # print(help(type(enc)))\n",
    "\n",
    "\n",
    "# label_encoder = enc.fit(titanic_X[:, 2])\n",
    "# print(\"Categorical classes:\", label_encoder.classes_)\n",
    "# integer_classes = label_encoder.transform(label_encoder.classes_)\n",
    "# print (\"Integer classes:\", integer_classes)\n",
    "# # t = label_encoder.transform(titanic_X[:, 2])\n",
    "# # titanic_X[:, 2] = t\n",
    "# print ('Feature names:',feature_names)\n",
    "# print ('Features for instance number 12:',titanic_X[12], titanic_y[12])\n",
    "\n",
    "# ##Now to convert the passenger class. Since we have three different categories, we cannot convert to binary values (and using 0,1,2 values would imply an order, something we do not want). We use OneHotEncoder to get three different attributes:\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# #rjm49 - start by converting the pclass labels (column 0) into integer labels\n",
    "# lab_enc = LabelEncoder()\n",
    "# print(titanic_X[:,0])\n",
    "# new_features = lab_enc.fit_transform(titanic_X[:, 0])\n",
    "# print(\"unique new features\", np.unique(new_features))\n",
    "\n",
    "# print(\"Categorical classes:\", label_encoder.classes_)\n",
    "# integer_classes = label_encoder.transform(label_encoder.classes_)#.reshape(3, 1)\n",
    "# print(\"Integer classes:\", integer_classes.shape)\n",
    "\n",
    "# #rjm49 - get the integer labels for all the passengers\n",
    "# int_labels = label_encoder.transform(titanic_X[:,0]).reshape(-1,1)\n",
    "# print(\"int_labels:\\n\", int_labels)\n",
    "\n",
    "# #rjm49 - next, convert integer labels into \"one hot\" flags e.g. 0->[1 0 0], 1->[0 1 0], 2->[0 0 1]\n",
    "# onehot_enc = OneHotEncoder()\n",
    "# onehot_enc.fit(integer_classes.reshape(3, 1)) #rjm49 - note that this expects the classes to be passed in as a 3x1\n",
    "\n",
    "# #create a sparse matrix with three columns, each one indicating if the instance belongs to the class\n",
    "# new_features = onehot_enc.transform( int_labels ).toarray()\n",
    "# print(\"new_features are:\\n\", new_features)\n",
    "\n",
    "# print(titanic_X.shape)\n",
    "# print(new_features.shape)\n",
    "# titanic_X = np.concatenate([titanic_X, new_features.reshape(-1,1)], axis = 1) # rjm49, add the three new columns\n",
    "\n",
    "# #Delete the old columns which have now been converted\n",
    "# titanic_X = np.delete(titanic_X, [0], axis=1)\n",
    "# # Update feature names\n",
    "# feature_names = ['age', 'sex', 'class']# 'first_class', 'second_class', 'third_class']\n",
    "# # Convert to numerical values\n",
    "# titanic_X = titanic_X.astype(float)\n",
    "# titanic_y = titanic_y.astype(float)\n",
    "\n",
    "# print(titanic_X)\n",
    "\n",
    "# print ('New feature names:',feature_names)\n",
    "# print ('Values:',titanic_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_X, titanic_y, test_size=0.25, random_state=33)\n",
    "print(X_train[0:10])\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a new DecisionTreeClassifier and use the fit method of the classifier to do the learning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=5)\n",
    "clf_dt = clf_dt.fit(X_train,y_train)\n",
    "print(clf_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree we have built represents a series of decisions based on the training data. To classify an instance, we should answer the question at each node. For example, at our root node, the question is: Is sex<=0.5? (are we talking about a woman?). If the answer is yes, you go to the left child node in the tree; otherwise you go to the right child node. You keep answering questions (was she in the third class?, was she in the first class?, and was she below 13 years old?), until you reach a leaf. When you are there, the prediction corresponds to the target class that has most instances (that is if the answers are given to the previous questions). In our case, if she was a woman from second class, the answer would be 1 (that is she survived), and so on. Let's drawit, using pyplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "dot_data = io.StringIO() \n",
    "sk.tree.export_graphviz(clf_dt, out_file=dot_data, feature_names=titanic_X.columns, filled=True) \n",
    "(graph,) = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "from IPython.core.display import Image \n",
    "Image(graph.create_png())\n",
    "\n",
    "# sk.tree.plot_tree(clf_dt.fit(titanic_X, titanic_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure Accuracy, precision, recall, f1 in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X,y,clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "    y_pred=clf.predict(X)   \n",
    "    if show_accuracy:\n",
    "        print( \"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred)),\"\\n\")\n",
    "\n",
    "    if show_classification_report:\n",
    "        print( \"Classification report\")\n",
    "        print( metrics.classification_report(y,y_pred),\"\\n\")\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print( \"Confusion matrix\")\n",
    "        print( metrics.confusion_matrix(y,y_pred),\"\\n\")\n",
    "        \n",
    "measure_performance(X_train,y_train,clf, show_classification_report=False, show_confusion_matrix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use an extreme case of cross- validation, named leave-one-out cross-validation. For each instance in the training sample, we train on the rest of the sample, and evaluate the model built on the only instance left out. After performing as many classifications as training instances, we calculate the accuracy simply as the proportion of times our method correctly predicted the class of the left-out instance, and found it is a little lower (as we expected) than the resubstitution accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, LeaveOneOut #model_selection replaces cross_validation\n",
    "from scipy.stats.stats import sem\n",
    "\n",
    "def loo_cv(X_train,y_train,clf):\n",
    "    # Perform Leave-One-Out cross validation\n",
    "    # We are performing 1313 classifications!\n",
    "    loo = LeaveOneOut()\n",
    "    scores=np.zeros(len(X_train)) #rjm49 - list of zeroes that's as long as X_train\n",
    "    #for train_index,test_index in loo(X_train):\n",
    "#     first = True\n",
    "    y_hats = []\n",
    "    for train_index,test_index in loo.split(X_train):\n",
    "#         if first:\n",
    "#             print(train_index, test_index)\n",
    "#             first = False\n",
    "        X_train_cv, X_test_cv= X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_test_cv= y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        clf = clf.fit(X_train_cv,y_train_cv)\n",
    "        y_pred=clf.predict(X_test_cv)\n",
    "        y_hats.append(int(y_pred))\n",
    "#         print(y_test_cv.astype(int), y_pred.astype(int))\n",
    "        scores[test_index]=sk.metrics.accuracy_score(y_test_cv.astype(int), y_pred.astype(int))\n",
    "#         f1s[test_index]=sk.metrics.f1_score(y_test_cv.astype(int), y_pred.astype(int))\n",
    "    f1 = sk.metrics.f1_score(y_train, y_hats)\n",
    "    r = sk.metrics.recall_score(y_train, y_hats)\n",
    "    p = sk.metrics.precision_score(y_train, y_hats)    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from scipy.stats import skew\n",
    "    print(\"skew:\",skew(y_train))\n",
    "    print(\"Mean acc: {0:.3f} (+/-{1:.3f})\".format(np.mean(scores), np.std(scores)))\n",
    "    print(\"Macro p/r/f1: {:.3f}/{:.3f}/{:.3f}\".format(p,r,f1))\n",
    "    print(confusion_matrix(y_train, y_hats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Home cooked LOOCV:\")\n",
    "loo_cv(titanic_X, titanic_y, clf_dt)\n",
    "\n",
    "print(\"\\nUse sklearn's cross_val_score util:\")\n",
    "cv_scores = cross_val_score(clf_dt, titanic_X, titanic_y, cv=LeaveOneOut())#.split(titanic_X))\n",
    "print(cv_scores.mean(), cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common criticism to decision trees is that once the training set is divided after answering a question, it is not possible to reconsider this decision. For example, if we divide men and women, every subsequent question would be only about men or women, and the method could not consider another type of question (say, age less than a year, irrespective of the gender). Random Forests try to introduce some level of randomization in each step, proposing alternative trees and combining them to get the final prediction. These types of algorithms that consider several classifiers answering the same question are called ensemble methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=10, max_depth=5, min_samples_leaf=5)\n",
    "clf_rf = clf_rf.fit(X_train,y_train)\n",
    "loo_cv(X_train,y_train,clf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate performance on future data, evaluate on the training set and test on the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision tree:\")\n",
    "measure_performance(X_test,y_test,clf_dt)\n",
    "\n",
    "print(\"Random forest:\")\n",
    "measure_performance(X_test,y_test,clf_rf)\n",
    "\n",
    "#rjm49 -try your luck here... \n",
    "my_age = 19 #your age in Earth years\n",
    "my_gender = 0 #[0, 1] for female, male\n",
    "my_class = 0 #[0, 1, 2] for 1st, 2nd, 3rd\n",
    "me = np.array([my_class, my_age, my_gender]).reshape(1,-1)\n",
    "\n",
    "print(\"DTree thinks I'm {:.3f} likely to survive.\".format(clf_dt.predict_proba(me)[0][1]))\n",
    "print(\"RForest thinks I'm {:.3f} likely to survive.\".format(clf_rf.predict_proba(me)[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
