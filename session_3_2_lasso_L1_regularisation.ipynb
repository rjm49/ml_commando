{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation (L1 or Lasso)\n",
    "## Cambridge ML Commando Course\n",
    "\n",
    "In this notebook we will:\n",
    "- create noisy data based on a pure signal\n",
    "- create regressors with various non-linear features\n",
    "- test their fits and plot them, along with printing their cross-validation scores\n",
    "- implement lasso regression to smooth out overfit, inspect how it works\n",
    "- plot a validation curve for our lasso regressor\n",
    "- plot learning curves for all our regressors\n",
    "- Check the correlation matrix of the data for clues about performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.array([0, 1, 2, 3])\n",
    "X = X.reshape(-1,1)\n",
    "true_fun = lambda x : np.sin(x)\n",
    "y = true_fun(X)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pure = np.arange(0,2*3.1415,0.1)\n",
    "\n",
    "true_fun = lambda x : np.sin(x) # Why not try a different function?\n",
    "\n",
    "np.random.seed(666)\n",
    "X = np.sort(random.choice(X_pure, size=30, replace=False))\n",
    "\n",
    "y_pure = np.sin(X_pure)\n",
    "y = true_fun(X) + np.random.randn(len(X))*0.25\n",
    "print(\"y values:\\n\",y)\n",
    "print(len(y))\n",
    "\n",
    "plt.plot(X_pure, true_fun(X_pure), linestyle=\"--\")\n",
    "plt.scatter(X,y)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "scaler15 = StandardScaler()\n",
    "poly15 = PolynomialFeatures(15) # gives us 16 polynomial features, x^0, x^1, x^2 ... x^15\n",
    "\n",
    "steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (\"reg\",LinearRegression())\n",
    "]\n",
    "reg15 = Pipeline( steps )\n",
    "\n",
    "X=X.reshape(-1,1)\n",
    "reg15.fit(X,y)\n",
    "\n",
    "lasso_regressor = Lasso(alpha=0.00055, tol=0.01, max_iter=100000)\n",
    "\n",
    "steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (\"reg\", lasso_regressor) # put your lasso regressor here\n",
    "]\n",
    "lasso15 = Pipeline( steps )\n",
    "lasso15.fit(X,y)\n",
    "\n",
    "print(\"Quindecic model\")\n",
    "scores15 = cross_val_score(reg15, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "print(-np.mean(scores15), np.std(scores15))\n",
    "\n",
    "print(\"Lasso regression regularisation\")\n",
    "lasso_scores = cross_val_score(lasso15, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "print(-np.mean(lasso_scores), np.std(lasso_scores))\n",
    "\n",
    "plt.plot(X_pure, true_fun(X_pure), linestyle=\"--\", color=\"k\", label=\"true\")\n",
    "plt.plot(X_pure, list(reg15.predict(X_pure.reshape(-1,1))), label=\"quindecic (15)\")\n",
    "plt.plot(X_pure, lasso15.predict(X_pure.reshape(-1,1)), label=\"lasso (15)\")\n",
    "plt.scatter(X, y, label=\"data\")\n",
    "plt.gcf().set_size_inches(10,8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "How did that work?  Was it what you were expecting to see?\n",
    "Maybe it's time to investigate what effects changing *alpha* will have...\n",
    "\n",
    "PS: If you are getting convergence warnings, you may need to increase the Lasso object's tolerance (tol) to some larger value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pure = X_pure.reshape(-1,1)\n",
    "alphas = np.logspace(-5, 0, 7)\n",
    "ax = plt.figure().gca()\n",
    "ax.plot(X_pure, true_fun(X_pure), linestyle=\"--\", c=\"k\", label=\"true\")\n",
    "ax.scatter(X, y)\n",
    "for a in alphas:\n",
    "    steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (\"reg\",Lasso(alpha=a, tol=0.01, max_iter=1000000))\n",
    "    ]\n",
    "    new_ridge = Pipeline( steps )\n",
    "    new_ridge.fit(X,y)\n",
    "#     new_ridge_scores = cross_val_score(new_ridge, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    ax.plot(X_pure, new_ridge.predict(X_pure), label=a)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.legend(title=\"alpha\")\n",
    "plt.gcf().set_size_inches(10,8)\n",
    "plt.title('Ridge (15): Fit to datapoints under increasing regularisation')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coefs = []\n",
    "alphas = np.logspace(-5, 0, 30)\n",
    "\n",
    "powers = None\n",
    "scores = []\n",
    "for a in alphas: # you can use the alphas you defined earlier\n",
    "    steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (\"reg\",Lasso(alpha=a, tol=0.01, max_iter=1000000))\n",
    "    ]\n",
    "    new_lasso = Pipeline( steps )\n",
    "    new_lasso.fit(X,y)\n",
    "    score = numpy.mean(cross_val_score(new_lasso, X, y, scoring=\"neg_mean_squared_error\", cv=10))\n",
    "    scores.append(score)\n",
    "    coefs.append(new_lasso.named_steps[\"reg\"].coef_)\n",
    "    if powers is None: # get these for use in labelling the series, so we can see what Poly features we have\n",
    "        powers = new_lasso.named_steps[\"poly\"].powers_\n",
    "\n",
    "coefs = numpy.array(coefs).T # transpose to get a time-series of coefs per row\n",
    "    \n",
    "plt.gcf().set_size_inches(10,8)\n",
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Coefficient weight')\n",
    "\n",
    "axsc = ax.twinx()\n",
    "print(alphas.shape, len(coefs))\n",
    "\n",
    "\n",
    "for power,coef_vals in zip(powers, coefs):\n",
    "    ax.plot(alphas, coef_vals, label=\"$x^{}$\".format(\"{\"+str(int(power))+\"}\"))\n",
    "ax.legend()\n",
    "axsc.plot(alphas, scores, linestyle=\"--\", label=\"-ve MSE\")\n",
    "axsc.set_ylabel('-ve MSE score (higher=better!)')\n",
    "axsc.legend()\n",
    "    \n",
    "plt.xlabel('alpha')\n",
    "plt.title('Lasso coefficients as a function of the regularisation param')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see how coefficients are being dropped to zero as you increase alpha?  This removes the corresponding features from the model, making it *sparse*.\n",
    "\n",
    "Have a look at some more characteristics of the lasso regressor below, using validation and learning curves.  You dont need to fill in any of this code, but you might want to take the best value and apply it to your code above, and see if it improves the fit and the cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "def plot_validation_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, param_name=\"C\", param_range = np.logspace(-5, 0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator, X, y, \n",
    "        param_name=param_name, \n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        param_range=param_range,\n",
    "        cv=cv, n_jobs=n_jobs)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "            \n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "    lw = 2\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "\n",
    "    plt.gcf().set_size_inches(10,5)\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X15=scaler15.transform(poly15.transform(X)) # here we have to transform the X values ourselves, without relying on a pipeline\n",
    "plot_validation_curve(Lasso(alpha=0.00055, tol=0.01, max_iter=1000000), \"Lasso15\", X15, y, None, cv=10, n_jobs=-1, param_name=\"alpha\", param_range=np.logspace(-5, 0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, scoring=\"neg_mean_squared_error\", train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", alpha=0.5,\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", alpha=0.5,\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.gcf().set_size_inches(10,5)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0) # works better for small dataset\n",
    "# cv=10\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 20) # these are fractions of the total dataset!\n",
    "print(\"Using following proportions of data:\",train_sizes)\n",
    "plot_learning_curve(reg15, \"Overfit (15)\", X, y, (-1e8,.5e7), cv=cv, n_jobs=4, train_sizes=train_sizes)\n",
    "plot_learning_curve(lasso15, \"Lasso (15)\", X, y, (-10,1), cv=cv, n_jobs=4, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End note\n",
    "It seems like the lasso regressor struggles a bit with this data.  You need to set the *alpha* value very small to avoid overly sparse regularisation, which leads to underfit.  This in turn means we needed to increase the tolerance and/or iterations on the estimator, otherwise it does not converge properly.\n",
    "\n",
    "Can you think why this might be?  Have a look at the correlation matrix (below) for clues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_corrs = numpy.corrcoef(X15[:,1:].T) # remove first feature (always zero) and transpose (one feature per row)\n",
    "numpy.set_printoptions(precision=2)\n",
    "for fc in feat_corrs:\n",
    "    print(fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
