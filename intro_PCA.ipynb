{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dimensionality Reduction with PCA\n",
    "Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "remove_setosa = False\n",
    "\n",
    "if remove_setosa:\n",
    "    indices_no_setosa = (iris.target != 0) #[TRUE for non-setosa else FALSE]\n",
    "    X_iris = iris.data[indices_no_setosa]\n",
    "    y_iris = iris.target[indices_no_setosa]\n",
    "else:\n",
    "    X_iris = iris.data\n",
    "    y_iris = iris.target\n",
    "\n",
    "numpy.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Test set will be the 25% taken randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.10, random_state=33)\n",
    "\n",
    "colormarkers = [ ['red','s'], ['greenyellow','o'], ['blue','x']]\n",
    "\n",
    "# Scale the features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px,py = X_train[:,0], X_train[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.figure().gca()\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "ax.scatter(px,py, alpha=0.4, c=y_train)\n",
    "ax.set_xlabel('P.length')\n",
    "ax.set_ylabel('S.length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "txd = pca.fit_transform(X_train[:,[0,2]])\n",
    "\n",
    "def draw_vector(j, v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "    ax.annotate(\"PC{}\".format(1+j), v1)\n",
    "\n",
    "# plot data\n",
    "px,py = X_train[:,0],X_train[:,2] \n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(px,py, alpha=0.4, c=y_train)\n",
    "print(\"Our PCA scale factors and direction vectors are:\")\n",
    "for i, (var, direction_vec) in enumerate(zip(pca.explained_variance_, pca.components_)):\n",
    "    print(var, direction_vec)\n",
    "    l = np.sqrt(var)\n",
    "    v = direction_vec * l #np.sqrt(length)\n",
    "#     print(v)\n",
    "    draw_vector(i, pca.mean_, pca.mean_ + v)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_pcs = False\n",
    "print(\"Replot, with rotated X,y values,\")\n",
    "print(\"And project points onto PC1:\")\n",
    "px, py = txd[:,0], txd[:,1]\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "bg_alpha = 0.2 if overlay_pcs else 0.5\n",
    "ax.scatter(px,py, alpha=bg_alpha, c=y_train)\n",
    "if overlay_pcs:\n",
    "    ax.scatter(px,[0 for p in px], alpha=0.5, c=y_train)\n",
    "    ax.scatter([0 for p in py],py, alpha=0.5, c=y_train)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "for i,xv in enumerate(pca.explained_variance_ratio_):\n",
    "    print(\"PC{} explains {:.2f}% of variance\".format(1+i,xv*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition of the Covariance Matrix\n",
    "Underneath the sklearn wrapper, PCA is doing SVD on the Covariance Matrix of our features.  Here we do the same steps ourselves.  Compare the U,S,V values to the scale and direction values in the section above.  They should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train[:,[0,2]]\n",
    "print(M.shape)\n",
    "\n",
    "covariance_mx = numpy.cov(M.T)\n",
    "print(\"Numpy cov:\\n\",covariance_mx)\n",
    "\n",
    "# Do not confuse the covariance matrix with the correlation matrix (which is normalised)!\n",
    "# corr_mx = numpy.corrcoef(M.T)\n",
    "# print(\"Numpy corr:\\n\",corr_mx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(covariance_mx)\n",
    "\n",
    "# # We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(covariance_mx)))\n",
    "ax.set_yticks(np.arange(len(covariance_mx)))\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(covariance_mx.shape[0]):\n",
    "    for j in range(covariance_mx.shape[0]):\n",
    "        text = ax.text(j, i, round(covariance_mx[i, j],3),\n",
    "                       ha=\"center\", va=\"center\", color=\"grey\")\n",
    "\n",
    "print()\n",
    "print(\"Diagonals of the covariance matrix are the variances of individual features\")\n",
    "print(\"Off-diagonals show covariances: how much two features vary together\")\n",
    "        \n",
    "ax.set_title(\"Covariance matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,V_T = sp.linalg.svd(covariance_mx)\n",
    "print(\"U matrix (PC directions)\")\n",
    "print(U)\n",
    "print(\"\\nS(igma) (scale values)\")\n",
    "print(\"This list is taken from the diagonal of the Sigma matrix, ordered by descending magnitude\")\n",
    "print(S)\n",
    "print(\"\\nV^T matrix (same as U)\")\n",
    "print(V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with values taken from original PCA operation above:\n",
    "\n",
    "    0.1336 [0.5949 0.8038]\n",
    "    0.0087 [-0.8038  0.5949]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how the PCs explain the data, with a Scree plot\n",
    "Now we compress the full set of Iris data (4 components) using PCA.\n",
    "We use the output to create a scree plot which can be used to select an appropriate dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca4d = PCA(n_components=4)\n",
    "pca4d.fit(X_train)\n",
    "\n",
    "cmps = pca4d.components_\n",
    "print(cmps)\n",
    "\n",
    "variance = pca4d.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "var=np.cumsum(np.round(pca4d.explained_variance_ratio_, decimals=3)*100)\n",
    "#var is cumulative sum of variance explained with [n] features\n",
    "\n",
    "ax = plt.figure(figsize=(8,6)).gca()\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Components')\n",
    "plt.title('Iris data, PCA Scree Plot')\n",
    "plt.ylim(70,100.5)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.plot([1,2,3,4],var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher dimensioned datasets\n",
    "Now we do some PCA and scree plots with the wine (13D) and Boston (14D if we include price) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = sklearn.datasets.load_wine()\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "k = X.shape[1]\n",
    "pca_kd = PCA(n_components=k)\n",
    "txd = pca_kd.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(txd[:,0], txd[:,1], c=y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Wine, reduced from 13D\")\n",
    "plt.show()\n",
    "variance = pca_kd.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(pca_kd.explained_variance_ratio_, decimals=3)*100)\n",
    "ax = plt.figure(figsize=(8,6)).gca()\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Components')\n",
    "plt.title('Wine Scree Plot')\n",
    "plt.ylim(0,100.5)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.plot(list(range(1,k+1)),var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = sklearn.datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "mms = sklearn.preprocessing.StandardScaler()\n",
    "X = mms.fit_transform(numpy.c_[X,y]) # concatenate X features with the price\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "k = X.shape[1]\n",
    "pca_kd = PCA(n_components=k)\n",
    "txd = pca_kd.fit_transform(X)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(txd[:,0], txd[:,1], c=y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Boston, reduced from 14D\")\n",
    "plt.show()\n",
    "variance = pca_kd.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(pca_kd.explained_variance_ratio_, decimals=3)*100)\n",
    "ax = plt.figure(figsize=(8,6)).gca()\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Components')\n",
    "plt.title('Boston Scree Plot')\n",
    "plt.ylim(0,100.5)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.plot(list(range(1,k+1)),var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we:\n",
    " - Tested PCA on a boring 2d dataset that we extracted from the Iris data\n",
    " - Plotted the PCs on the original dataset, got their directional and length values\n",
    " - Transformed the dataset and projected its points onto the PCs\n",
    " - Performed SVD outside of the PCA routine and verified that we get the same PCs\n",
    " - Used a scree plot to see how successive PCs add to the cumulative variance explained\n",
    " - Did PCA on a pair of higher dimensional datasets and made scree plots for them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
