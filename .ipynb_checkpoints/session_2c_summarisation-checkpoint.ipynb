{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Commando Course, Cambridge Computer Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2b Further NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook might not work properly without:\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000\n",
    "\n",
    "_One of the most successful applications of Naïve Bayes has been within the field\n",
    "of Natural Language Processing (NLP). NLP is a field that has been much related\n",
    "to machine learning, since many of its problems can be formulated as a classification task. Usually, NLP problems have important amounts of tagged data in the form of text documents. This data can be used as a training dataset for machine\n",
    "learning algorithms.\n",
    "In this section, we will use Naïve Bayes for text classification; we will have a set of text documents with their corresponding categories, and we will train a Naïve Bayes algorithm to learn to predict the categories of new unseen instances. This simple task has many practical applications; probably the most known and widely used one is spam filtering. In this section we will try to classify newsgroup messages using a dataset that can be retrieved from within scikit-learn. This dataset consists of around 19,000 newsgroup messages from 20 different topics ranging from politics and religion to sports and science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# print ('IPython version:', IPython.__version__)\n",
    "# print ('numpy version:', np.__version__)\n",
    "# print ('scikit-learn version:', sk.__version__)\n",
    "# print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the newsgroup Dataset, and explore its structure and data (this could take some time, especially if sklearn has to download the 14MB dataset from the Internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846 (18846,)\n",
      "0 alt.atheism\n",
      "1 comp.graphics\n",
      "2 comp.os.ms-windows.misc\n",
      "3 comp.sys.ibm.pc.hardware\n",
      "4 comp.sys.mac.hardware\n",
      "5 comp.windows.x\n",
      "6 misc.forsale\n",
      "7 rec.autos\n",
      "8 rec.motorcycles\n",
      "9 rec.sport.baseball\n",
      "10 rec.sport.hockey\n",
      "11 sci.crypt\n",
      "12 sci.electronics\n",
      "13 sci.med\n",
      "14 sci.space\n",
      "15 soc.religion.christian\n",
      "16 talk.politics.guns\n",
      "17 talk.politics.mideast\n",
      "18 talk.politics.misc\n",
      "19 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "\n",
    "X = news.data\n",
    "y = news.target\n",
    "y_labels = news.target_names\n",
    "\n",
    "print(len(X), y.shape) # X is a list, y is a numpy array\n",
    "for yt in set(y): print(yt, y_labels[yt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at a random instance, you will see the content of a newsgroup message, and you can get its corresponding category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mangoe@cs.umd.edu (Charley Wingate)\n",
      "Subject: Re: A Little Too Satanic\n",
      "Organization: U of Maryland, Dept. of Computer Science, Coll. Pk., MD 20742\n",
      "Lines: 43\n",
      "\n",
      "Jon Livesey writes:\n",
      "\n",
      ">So why do I read in the papers that the Qumram texts had \"different\n",
      ">versions\" of some OT texts.   Did I misunderstand?\n",
      "\n",
      "Reading newspapers to learn about this kind of stuff is not the best idea in\n",
      "the world.  Newspaper reporters are notoriously ignorant on the subject of\n",
      "religion, and are prone to exaggeration in the interests of having a \"real\"\n",
      "story (that is, a bigger headline).\n",
      "\n",
      "Let's back up to 1935.  At this point, we have the Masoretic text, the\n",
      "various targums (translations/commentaries in aramaic, etc.), and the\n",
      "Septuagint, the ancient greek translation.  The Masoretic text is the\n",
      "standard Jewish text and essentially does not vary.  In some places it has\n",
      "obvious corruptions, all of which are copied faithfully from copy to copy.\n",
      "These passages in the past were interpreted by reference to the targums and\n",
      "to the Septuagint.\n",
      "\n",
      "Now, the septuagint differs from the masoretic text in two particulars:\n",
      "first, it includes additional texts, and second, in some passages there are\n",
      "variant readings from the masoretic text (in addition to \"fixing\"/predating\n",
      "the various corrupted passages).  It must be emphasized that, to the best of\n",
      "my knowledge, these variations are only signifcant to bible scholars, and\n",
      "have little theological import.\n",
      "\n",
      "The dead sea scroll materials add to this an ancient *copy* of almost all of\n",
      "Isaiah and fragments of various sizes of almost all other OT books.  There\n",
      "is also an abundance of other material, but as far as I know, there is no\n",
      "sign there of any hebrew antecdent to the apocrypha (the extra texts in the\n",
      "septuagint).  As far as analysis has proceeded, there are also variations\n",
      "between the DSS texts and the masoretic versions.  These tend to reflect the\n",
      "septuagint, where the latter isn't obviously in error.  Again, though, the\n",
      "differences (thus far) are not significant theologically.  There is this big\n",
      "expectation that there are great theological surprises lurking in the\n",
      "material, but so far this hasn't happened.\n",
      "\n",
      "The DSS *are* important because there is almost no textual tradition in the\n",
      "OT, unlike for the NT.\n",
      "-- \n",
      "C. Wingate        + \"The peace of God, it is no peace,\n",
      "                  +    but strife closed in the sod.\n",
      "mangoe@cs.umd.edu +  Yet, brothers, pray for but one thing:\n",
      "tove!mangoe       +    the marv'lous peace of God.\"\n",
      "\n",
      "0 alt.atheism\n"
     ]
    }
   ],
   "source": [
    "rand_ix = np.random.randint(0, len(news.data))\n",
    "print (news.data[rand_ix])\n",
    "category = news.target[rand_ix]\n",
    "category_name = news.target_names[category]\n",
    "print (category, category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<learner><head sortkey=\"TR798*0100*2000*01\">\n",
      "  <candidate><personnel><language>Chinese</language><age>16-20</age></personnel><score>24.0</score></candidate>\n",
      "  <text>\n",
      "     <answer1>\n",
      "       <question_number>1</question_number>\n",
      "       <exam_score>3.2</exam_score>\n",
      "         <coded_answer>\n",
      "          <p>Dear Mr Ryan<NS type=\"RP\"><i>.</i><c>,</c></NS></p>\n",
      "          <p>Thanks for <NS type=\"DD\"><i>you</i><c>your</c></NS> letter. I am so <NS type=\"RJ\"><i>exciting</i><c>excited</c></NS> that I have won the first prize. I will give you all <NS type=\"MD\"><c>the</c></NS> information you need and ask some questions.</p>\n",
      "          <p>I <NS type=\"TV\"><i>could</i><c>can</c></NS> only travel <NS type=\"RT\"><i>on</i><c>in</c></NS> July. As you know, I am a student and the nearest holiday is <NS type=\"MD\"><c>the</c></NS> summer holiday. But I have booked a flight <NS type=\"UT\"><i>to</i></NS> home at the beginning of <NS type=\"S\"><i>Auguest</i><c>August</c></NS>. And also I would like to go <NS type=\"RT\"><i>on</i><c>in</c></NS> summer.</p>\n",
      "          <p><NS type=\"MT\"><i>The</i><c>Concerning the</c></NS> accommodation<NS type=\"MP\"><c>,</c></NS> I would like to <NS type=\"RV\"><i>live</i><c>stay</c></NS> in tents<NS type=\"RP\"><i>. Because</i><c>, because</c></NS> I <NS type=\"TV\"><i>never <NS type=\"RV\"><i>live</i><c>stay</c></NS></i><c>have never stayed</c></NS> in <NS type=\"MD\"><c>a</c></NS> <NS type=\"AGN\"><i>tents</i><c>tent</c></NS> before. I think it <NS type=\"TV\"><i>is</i><c>will be</c></NS> great and I want to try it.</p>\n",
      "          <p>I like doing sports. I would like to play basketball and golf when I am at the Camp. I play basketball a lot and I am a member of our college <NS type=\"RN\"><i>term</i><c>team</c></NS>. But I am not very good at golf.</p>\n",
      "          <p>And also I want to ask some questions. What clothes should I <NS type=\"TV\"><i>taken</i><c>take</c></NS>? How much money should I <NS type=\"TV\"><i>taken</i><c>take</c></NS>? And how <NS type=\"TV\"><i>could</i><c>can</c></NS> we meet at the airport? I am looking forward <NS type=\"MT\"><c>to</c></NS> your reply.</p>\n",
      "          <p>Yours sincerely.</p>\n",
      "         </coded_answer>\n",
      "     </answer1>\n",
      "     <answer2>\n",
      "       <question_number>2</question_number>\n",
      "       <exam_score>2.3</exam_score>\n",
      "         <coded_answer>\n",
      "          <p>As our class is going to <NS type=\"RV\"><i>mark</i><c>make</c></NS> a short video about daily life at college, I <NS type=\"TV\"><i>write</i><c>am writing</c></NS> this report to suggest some lessons and activities which should be filmed.</p>\n",
      "          <p>1. English lesson. Because <NS type=\"UQ\"><i>all</i></NS> students in <NS type=\"MD\"><c>the</c></NS> English class are from all over the world<NS type=\"RP\"><i>. We</i><c>, we</c></NS> can talk <NS type=\"MT\"><c>about</c></NS> everybody's <NS type=\"FN\"><i>feeling</i><c>feelings</c></NS> <NS type=\"MT\"><c>about</c></NS> living and <NS type=\"FV\"><i>study</i><c>studying</c></NS> <NS type=\"RT\"><i>at</i><c>in</c></NS> a foreign country.</p>\n",
      "          <p>2. Computing lesson and computer room. <NS type=\"S\"><i>Nowdays</i><c>Nowadays</c></NS> <NS type=\"MD\"><c>the</c></NS> <NS type=\"RP\"><i>internet</i><c>Internet</c></NS> <NS type=\"RV\"><i>makes</i><c>brings</c></NS> us closer and closer. We can get all <NS type=\"UA\"><i>what</i></NS> we want on <NS type=\"MD\"><c>the</c></NS> <NS type=\"RP\"><i>internet</i><c>Internet</c></NS>. It's one of the most important things in our life now.</p>\n",
      "          <p>3. <NS type=\"S\"><i>Liabrary</i><c>Library</c></NS>. We not only borrow books from <NS type=\"MD\"><c>a</c></NS> <NS type=\"S\"><i>liabrary</i><c>library</c></NS> but also study at <NS type=\"MD\"><c>a</c></NS> <NS type=\"S\"><i>liabrary</i><c>library</c></NS>. <NS type=\"MD\"><i>Library</i><c>The library</c></NS> is very important in our daily life.</p>\n",
      "          <p>4. Canteen. <NS type=\"RP\"><i>Everyday</i><c>Every day</c></NS> we go to <NS type=\"MD\"><c>the</c></NS> canteen <NS type=\"FV\"><i>have</i><c>to have</c></NS> lunch, no matter <NS type=\"MC\"><c>whether</c></NS> you <NS type=\"TV\"><i>bought</i><c>buy</c></NS> food from there or you take your own food.</p>\n",
      "          <p>5. Football. What do you do after class? <NS type=\"DN\"><i>Joging</i><c>Jogging</c></NS> or <NS type=\"UV\"><i>doing</i></NS> some sports? You can't forget football.</p>\n",
      "          <p><NS type=\"AGA\"><i>These</i><c>This</c></NS> <NS type=\"AGV\"><i>are</i><c>is</c></NS> what I think should be filmed. If any of you have other <NS type=\"FN\"><i>suggestion</i><c>suggestions</c></NS>, we can discuss <NS type=\"MA\"><c>this</c></NS> again. But I think these five <NS type=\"AGN\"><i>lesson</i><c>lessons</c></NS> or activities are <NS type=\"MD\"><c>the</c></NS> most common in our daily life at college.</p>\n",
      "         </coded_answer>\n",
      "     </answer2>\n",
      "  </text>\n",
      "</head></learner>\n",
      "\n",
      "<Element 'learner' at 0x7f6084b5ec28>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "\n",
    "xml = open(\"./data/doc2.xml\",\"r\").read()\n",
    "print(xml)\n",
    "\n",
    "xml = \"./data/doc2.xml\"\n",
    "def parse_fce_doc(xmlfile):\n",
    "  \n",
    "    # create element tree object\n",
    "    tree = ET.parse(xmlfile)\n",
    "  \n",
    "    # get root element\n",
    "    root = tree.getroot()\n",
    "    print(root)\n",
    "  \n",
    "    # create empty list for news items\n",
    "    newsitems = []\n",
    "\n",
    "#     for ingredient in root.iter('p'):\n",
    "#         print(ingredient.child)\n",
    "    # iterate news items\n",
    "    for item in root.findall('p'):\n",
    "        print(\"item\")\n",
    "        print(item)\n",
    "#         # empty news dictionary\n",
    "#         news = {}\n",
    "  \n",
    "#         # iterate child elements of item\n",
    "#         for child in item:\n",
    "  \n",
    "#             # special checking for namespace object content:media\n",
    "#             if child.tag == '{http://search.yahoo.com/mrss/}content':\n",
    "#                 news['media'] = child.attrib['url']\n",
    "#             else:\n",
    "#                 news[child.tag] = child.text.encode('utf8')\n",
    "  \n",
    "#         # append news dictionary to news items list\n",
    "        newsitems.append(news)\n",
    "      \n",
    "    # return news items list\n",
    "    return newsitems\n",
    "\n",
    "news = parse_fce_doc(xml)\n",
    "print(len(news))\n",
    "for new in news:\n",
    "    print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, tokenize, PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "\n",
    "# stemmed_data= []\n",
    "# stop_ct = 0\n",
    "# for doc in news.data:\n",
    "#     new_doc=[]\n",
    "#     l_of_s = nltk.sent_tokenize(doc)\n",
    "# #     print(l_of_s)\n",
    "#     for s in l_of_s:\n",
    "#         rawlow = nltk.wordpunct_tokenize(s)\n",
    "# #         print(low)\n",
    "#         low = [ps.stem(w) for w in rawlow if (w not in stop_words)]\n",
    "#         stop_ct += (len(rawlow)-len(low))\n",
    "#         new_s = \" \".join(low)\n",
    "#         new_s = new_s.replace(\"-\",\"\").replace(\".\",\"\")\n",
    "#         new_doc.append(new_s)\n",
    "# #     print(new_doc)\n",
    "#     new_doc = \" \".join(new_doc)\n",
    "# #     print(new_doc)\n",
    "#     stemmed_data.append(new_doc)\n",
    "# print(\"stopped {} words\".format(stop_ct))\n",
    "\n",
    "stemmed_data = news.data\n",
    "\n",
    "SPLIT_PERC = 0.75\n",
    "split_size = int(len(stemmed_data)*SPLIT_PERC)\n",
    "X_train = stemmed_data[:split_size]\n",
    "X_test = stemmed_data[split_size:]\n",
    "y_train = news.target[:split_size]\n",
    "y_test = news.target[split_size:]\n",
    "\n",
    "#stopped raw words: 1840255\n",
    "#stopped stemmed words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mangoe@cs.umd.edu (Charley Wingate)\n",
      "Subject: Re: A Little Too Satanic\n",
      "Organization: U of Maryland, Dept. of Computer Science, Coll. Pk., MD 20742\n",
      "Lines: 43\n",
      "\n",
      "Jon Livesey writes:\n",
      "\n",
      ">So why do I read in the papers that the Qumram texts had \"different\n",
      ">versions\" of some OT texts.   Did I misunderstand?\n",
      "\n",
      "Reading newspapers to learn about this kind of stuff is not the best idea in\n",
      "the world.  Newspaper reporters are notoriously ignorant on the subject of\n",
      "religion, and are prone to exaggeration in the interests of having a \"real\"\n",
      "story (that is, a bigger headline).\n",
      "\n",
      "Let's back up to 1935.  At this point, we have the Masoretic text, the\n",
      "various targums (translations/commentaries in aramaic, etc.), and the\n",
      "Septuagint, the ancient greek translation.  The Masoretic text is the\n",
      "standard Jewish text and essentially does not vary.  In some places it has\n",
      "obvious corruptions, all of which are copied faithfully from copy to copy.\n",
      "These passages in the past were interpreted by reference to the targums and\n",
      "to the Septuagint.\n",
      "\n",
      "Now, the septuagint differs from the masoretic text in two particulars:\n",
      "first, it includes additional texts, and second, in some passages there are\n",
      "variant readings from the masoretic text (in addition to \"fixing\"/predating\n",
      "the various corrupted passages).  It must be emphasized that, to the best of\n",
      "my knowledge, these variations are only signifcant to bible scholars, and\n",
      "have little theological import.\n",
      "\n",
      "The dead sea scroll materials add to this an ancient *copy* of almost all of\n",
      "Isaiah and fragments of various sizes of almost all other OT books.  There\n",
      "is also an abundance of other material, but as far as I know, there is no\n",
      "sign there of any hebrew antecdent to the apocrypha (the extra texts in the\n",
      "septuagint).  As far as analysis has proceeded, there are also variations\n",
      "between the DSS texts and the masoretic versions.  These tend to reflect the\n",
      "septuagint, where the latter isn't obviously in error.  Again, though, the\n",
      "differences (thus far) are not significant theologically.  There is this big\n",
      "expectation that there are great theological surprises lurking in the\n",
      "material, but so far this hasn't happened.\n",
      "\n",
      "The DSS *are* important because there is almost no textual tradition in the\n",
      "OT, unlike for the NT.\n",
      "-- \n",
      "C. Wingate        + \"The peace of God, it is no peace,\n",
      "                  +    but strife closed in the sod.\n",
      "mangoe@cs.umd.edu +  Yet, brothers, pray for but one thing:\n",
      "tove!mangoe       +    the marv'lous peace of God.\"\n",
      "\n",
      "0 alt.atheism\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: kerney@ecn.purdue.edu (John Kerney)\n",
      "Subject: Re: FLYERS notes 4/17\n",
      "Keywords: FLYERS/Whalers summary\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Lines: 17\n",
      "\n",
      "\n",
      "\n",
      "Could someone post the Flyers record with and without Eric Lindros in\n",
      "the lineup\n",
      "\n",
      "\n",
      "I have a guy that is trying to compare the Quebec/Flyers trade to the \n",
      "\n",
      "Dallas/Minnesota trade in the NFL(Hershel Walker)\n",
      "\n",
      "I just need the stat to back up my point that Eric will be one of the next\n",
      "\n",
      "great players\n",
      "\n",
      "thanks\n",
      "\n",
      "john\n",
      "\n",
      "10 rec.sport.hockey\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: simun@unity.ncsu.edu (Josip NMI Simunovic)\n",
      "Subject: IBM 5272 ColorDisplay for 3270PC ?\n",
      "Article-I.D.: ncsu.1993Apr22.212557.27590\n",
      "Organization: NCSU\n",
      "Lines: 10\n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "\n",
      "\n",
      "I have one of these monitors. It appears to function OK, but is unhookable\n",
      "to anything standard (CGA,EGA,VGA) - it will plug in but gives fuzzy\n",
      "diagonal noise. \n",
      "I also have a graphics board that is apparently a 3270 graphic board\n",
      "(double card with 2 8-bit bus connectors, and a 9-pin female connector\n",
      "with a picture of monitor). I tried plugging these two into a standard AT\n",
      "to no avail. How can one connect these to (the monitor seems to\n",
      "be of relatively high quality, so I'm curious)? Any special drivers and/or\n",
      "setup needed - I can't locate any jumpers on the card.\n",
      "\n",
      "3 comp.sys.ibm.pc.hardware\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: livesey@solntze.wpd.sgi.com (Jon Livesey)\n",
      "Subject: Re: Islam & Dress Code for women\n",
      "Organization: sgi\n",
      "Lines: 12\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: solntze.wpd.sgi.com\n",
      "\n",
      "In article <1993Apr6.030734.28563@ennews.eas.asu.edu>, guncer@enuxha.eas.asu.edu (Selim Guncer ) writes:\n",
      ">\n",
      "> I wouldn't consider this quote as being exemplary of the Islamic \n",
      "> (TM) viewpoint though.  For all we know, the prophet's cousin and\n",
      "> the Fourth Khalif Hazret-i Ali may have said this after a \n",
      "> frustrating night with a woman.\n",
      "\n",
      "That's very interesting.    I wonder, are women's reactions\n",
      "recorded after a frustrating night with a man?   Is that\n",
      "considered to be important?\n",
      "\n",
      "jon.\n",
      "\n",
      "0 alt.atheism\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: sanjay@kin.lap.upenn.edu (Sanjay Sinha)\n",
      "Subject: Re: New to Motorcycles...\n",
      "Organization: University of Pennsylvania, Language Analysis Center\n",
      "Lines: 18\n",
      "Nntp-Posting-Host: kin.lap.upenn.edu\n",
      "\n",
      "In article <13612@news.duke.edu> infante@acpub.duke.edu (Andrew  Infante) writes:\n",
      ":>Curtis JAckson pens...\n",
      ":>\n",
      ":>\"MSF course...$140\"\n",
      ":\n",
      ":Shyah!\n",
      ":\n",
      ":The one here only costs $35!\n",
      ":\n",
      ":(Izzat a deal or what?! :)\n",
      "\n",
      "\n",
      "They are free in Philadelphia....   :-)\n",
      "\n",
      "-- \n",
      "   '81 CB650 \t\t\t\t\t\tDoD #1224\n",
      "\n",
      "\t   I would give my right arm to be ambidextrous!\n",
      "\n",
      "8 rec.motorcycles\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: rbutera@owlnet.rice.edu (Robert John Butera)\n",
      "Subject: Book Review Wanted\n",
      "Organization: Rice University\n",
      "Lines: 18\n",
      "\n",
      "I'm interested if anyone out here can point me towards a review of the\n",
      "following book in any scholarly Christian journal, whether it be\n",
      "conservative or liberal, Protestant or Catholic.\n",
      "\n",
      "_The_Lost_Years_of_Jesus_ (documentary evidence for Jesus' 17 year\n",
      "journey to the East), by Elizabeth Clare Prophet.  Supposedly this\n",
      "is a theory that was refuted in the past, and she has re-examined it.\n",
      "\n",
      "I thought this was just another novel book, but I saw it listed as\n",
      "a text for a class in religious studies here.  Also, the endorsements seem\n",
      "to come from some credible sources, so I'm wondering if scholars have\n",
      "reviewed it (or anyone on the net, for that matter).\n",
      "\n",
      "-- \n",
      "Rob Butera        |\n",
      "ECE Grad Student  |     \"Only sick music makes money today\" \n",
      "Rice University   |\n",
      "Houston, TX 77054 |               - Nietzsche, 1888\n",
      "\n",
      "15 soc.religion.christian\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: bil@okcforum.osrhe.edu (Bill Conner)\n",
      "Subject: Re: Death Penalty (was Re: Political Atheists?)\n",
      "Nntp-Posting-Host: okcforum.osrhe.edu\n",
      "Organization: Okcforum Unix Users Group\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "Lines: 14\n",
      "\n",
      "This is fascinating. Atheists argue for abortion, defend homosexuality\n",
      "as a means of population control, insist that the only values are\n",
      "biological and condemn war and capital punishment. According to\n",
      "Benedikt, if something is contardictory, it cannot exist, which in\n",
      "this case means atheists I suppose.\n",
      "I would like to understand how an atheist can object to war (an\n",
      "excellent means of controlling population growth), or to capital\n",
      "punishment, I'm sorry but the logic escapes me.\n",
      "And why just capital punishment, what is being questioned here, the\n",
      "propriety of killing or of punishment? What is the basis of the\n",
      "ecomplaint?\n",
      "\n",
      "Bill\n",
      "\n",
      "\n",
      "0 alt.atheism\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: m88max@tdb.uu.se (Max Brante)\n",
      "Subject: Atari Mono and VGA\n",
      "Organization: Department of Scientific Computing, Uppsala University\n",
      "Lines: 12\n",
      "\n",
      "Have anybody succeded in converting a atari monomchrome monitor into a\n",
      "mono VGA monitor. If so please let me know exactly how you did and what\n",
      "graphics card you used.\n",
      "\n",
      "\t/Thanx\n",
      "\n",
      "      __   __         _  _               \n",
      "     l  \\ /  l  ___  ( \\/ )          Max Brante   m88max@tdb.uu.se\n",
      "     l l l l l / _ \\  \\  /         \n",
      "     l l\\_/l l( (_) l /  \\\tInstitutionen f|r teknisk databehandling\n",
      "     l_l   l_l \\__l_l(_/\\_)               Uppsala Universitet  \n",
      "\n",
      "\n",
      "3 comp.sys.ibm.pc.hardware\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: David.Bernard@central.sun.com (Dave Bernard)\n",
      "Subject: Re: Question about Virgin Mary\n",
      "Reply-To: David.Bernard@central.sun.com\n",
      "Organization: Sun Microsystems\n",
      "Lines: 9\n",
      "\n",
      "In article 28782@athos.rutgers.edu, revdak@netcom.com (D. Andrew Kille) writes:\n",
      ">Just an observation- although the bodily assumption has no basis in\n",
      ">the Bible, Carl Jung declared it to be one of the most important pronouncements\n",
      ">of the church in recent years, in that it implied the inclusion of the \n",
      ">feminine into the Godhead.\n",
      "\n",
      "\n",
      "\n",
      "What did Jung mean by a \"Godhead?\"\n",
      "\n",
      "15 soc.religion.christian\n",
      "- - - -- - -- - ----- -- -- -\n",
      "From: lilley@v5.cgu.mcc.ac.uk (Chris Lilley)\n",
      "Subject: Re: XV problems\n",
      "Lines: 46\n",
      "Reply-To: C.C.Lilley@mcc.ac.uk\n",
      "Organization: Computer Graphics Unit, MCC\n",
      "\n",
      "\n",
      "In article <1r1iv3$cba@cc.tut.fi>, jk87377@lehtori.cc.tut.fi (Kouhia Juhana) writes:\n",
      "\n",
      ">Recent discussion about XV's problems were held in some newsgroup.\n",
      ">Here is some text users of XV might find interesting.\n",
      "\n",
      ">(I have also minor ideas for 24bit XV, e-mail me for them.)\n",
      "\n",
      "[Deleted for space; basically complaints that xv is an 8 bit program and that\n",
      "making several modifications to the RGB sliders is slow because of screen updates.]\n",
      "\n",
      "In reverse order:\n",
      "\n",
      "1) Try clicking in the auto-apply box to switch it off. Then make your mods. Then\n",
      "click on apply. There is no problem as stated; it has already been solved if you\n",
      "look carefully.\n",
      "\n",
      "2) Yes XV is an 8 bit program. This is not a bug. You can edit individual pallette\n",
      "entries or do global colour changes; crop, scale etc. Clearly the program must\n",
      "save out the *altered* image else all your work would be thrown away. So yes it\n",
      "saves out 8 bit images - of course!\n",
      "\n",
      "XV can import 24 bit images and quantises them down to 8 bits. This is a handy\n",
      "facility, not a bug.\n",
      "\n",
      "How would you suggest doing colour editing on a 24 bit file? How would you group\n",
      "'related' colours to edit them together? Only global changes could be done\n",
      "unless the software were very different and much more complicated.\n",
      "\n",
      "If you want to do colour editing on a 24 bit image, you need much more powerfull\n",
      "software - which is readily available commercially.\n",
      "\n",
      "And lastly, JPEG is a compression algorithm. It can be applied to any image of\n",
      "arbitrary bit depth. Again, this is not a bug. It is a way of saving disk space\n",
      ";-)\n",
      "\n",
      "Later,\n",
      "\n",
      "--\n",
      "Chris Lilley\n",
      "----------------------------------------------------------------------------\n",
      "Technical Author, ITTI Computer Graphics and Visualisation Training Project\n",
      "Computer Graphics Unit, Manchester Computing Centre, Oxford Road, \n",
      "Manchester, UK.  M13 9PL                        Internet: C.C.Lilley@mcc.ac.uk \n",
      "Voice: +44 (0)61 275 6045  Fax: +44 (0)61 275 6040 Janet: C.C.Lilley@uk.ac.mcc\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "1 comp.graphics\n",
      "- - - -- - -- - ----- -- -- -\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "rand_ixs = np.random.randint(0, len(stemmed_data), size=10)\n",
    "for rand_ix in rand_ixs:\n",
    "    print (stemmed_data[rand_ix])\n",
    "    category = news.target[rand_ix]\n",
    "    category_name = news.target_names[category]\n",
    "    print (category, category_name)\n",
    "    print(\"- - - -- - -- - ----- -- -- -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will serve to perform and evaluate a cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    print(\"underway...\")\n",
    "    k_cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=k_cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine learning algorithms can work only on numeric data...\n",
    "\n",
    "Inside the `sklearn.feature_extraction.text` module, there are three classes that can transform text into numeric features: `CountVectorizer`, `HashingVectorizer`, and `TfidfVectorizer`.\n",
    "The difference between them resides in the calculations they perform to derive the numeric features:\n",
    "- `CountVectorizer` basically creates a dictionary of words from the corpus. Then, each instance is converted to a vector of numeric features where each element will be the frequency of each word in the document.\n",
    "- `HashingVectorizer`, instead of constructing and maintaining the dictionary in memory, implements a hashing function that maps tokens into feature indexes, and then computes the count as in CountVectorizer. (Sadly seems intentionally broken at the time of writing!)\n",
    "- `TfidfVectorizer` works like CountVectorizer, with a more advanced calculation called \"Term Frequency - Inverse Document Frequency\" (TF-IDF). This is a statistic for measuring the importance of a word in a document or corpus. Intuitively, it looks for words that are more frequent in the current document, compared with their frequency across all documents. You can see this as a way to normalize the results and avoid words that are too frequent, and thus not useful to characterise the instances.\n",
    "\n",
    "We will create a Naïve Bayes classifier that is composed of a feature vectorizer and the actual Bayes classifier. We will use the MultinomialNB class from the `sklearn.naive_bayes` module. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "clf_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_2 = Pipeline([\n",
    "    ('vect', HashingVectorizer(alternate_sign=False)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_3 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk, tokenize, PorterStemmer\n",
    "nltk.download('treebank')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "\n",
    "entities_per_doc = [] # the i-th element gives a summary of the named entities found in the i-th document\n",
    "doc_n=0\n",
    "print(len(news.data))\n",
    "for doc in news.data:# nltk.corpus.treebank.tagged_sents():\n",
    "    doc_entities = []\n",
    "    doc = nltk.sent_tokenize(doc)\n",
    "    doc = [nltk.wordpunct_tokenize(sent) for sent in doc]\n",
    "    doc = [nltk.pos_tag(sent) for sent in doc]\n",
    "    for s in doc:        \n",
    "        chunx = ne_chunk(s, binary=True)\n",
    "        for chunk in chunx:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                doc_entities.extend([c[0] for c in chunk])\n",
    "    print(doc_entities)\n",
    "    entities_per_doc.append(\" \".join(doc_entities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-119416949571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrand_ixs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrand_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrand_ixs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "rand_ixs = np.random.randint(0, len(nes), size=10)\n",
    "for rand_ix in rand_ixs:\n",
    "    ent = nes[rand_ix]\n",
    "    print(ent)\n",
    "    print(\"-- - -- - -- - -- ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [clf_1, clf_2, clf_3]\n",
    "for clf in clfs:\n",
    "    print(\"for clf {}\".format(clf))\n",
    "    evaluate_cross_validation(clf, news.data, news.target, 5)\n",
    "\n",
    "# print(\">>> >>> >> USING NEs\")\n",
    "# clfs = [clf_1, clf_2, clf_3]\n",
    "# for clf in clfs:\n",
    "#     print(\"for clf {}\".format(clf))\n",
    "#     evaluate_cross_validation(clf, nes, news.target, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep the TF-IDF vectorizer but use a different regular expression to perform tokenization. The default regular expression: \"\\w\\w+\" considers alphanumeric characters and the underscore. Perhaps also considering the slash and the dot could improve the tokenization, and begin considering tokens as Wi-Fi and site.com. The new regular expression could be: \"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\". If you have queries about how to define regular expressions, please refer to the Python re module documentation. Let's try our new classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=\"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "clf_4a = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=\"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\",\n",
    "    )),\n",
    "    ('clf', LogisticRegression(multi_class='ovr')),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(news.data)\n",
    "evaluate_cross_validation(clf_4, news.data, news.target, 5)\n",
    "# evaluate_cross_validation(clf_4, stemmed_data, news.target, 5)\n",
    "evaluate_cross_validation(clf_4a, news.data, news.target, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter that we can use is stop_words: this argument allows us to pass a list of words we do not want to take into account, such as too frequent words, or words we do not a priori expect to provide information about the particular topic. Let's try to improve performance filtering the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e8cbbb46c3ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     ])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m clf_5a = Pipeline([\n\u001b[0m\u001b[1;32m     10\u001b[0m     ('vect', TfidfVectorizer(\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                 stop_words = stop_words,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "clf_5 = lambda c: Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words = stop_words,\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "clf_5a = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "clf_5b = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "clf_5c = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', LinearSVC(multi_class='ovr')),\n",
    "])\n",
    "\n",
    "clf_5d = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', SVC(kernel='linear')),\n",
    "])\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_5e = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', GradientBoostingClassifier()),\n",
    "])\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_5f = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "])\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_5g = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(20,5), early_stopping=True)),\n",
    "])\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_5h = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "#                 stop_words = stop_words,\n",
    "                stop_words = \"english\",\n",
    "                token_pattern='[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+',    \n",
    "    )),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=3, weights=\"distance\", n_jobs=-1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf_5, X_train, y_train, 5)\n",
    "# evaluate_cross_validation(clf_5, stemmed_data, news.target, 5)\n",
    "evaluate_cross_validation(clf_5a, X_train, y_train, 5)\n",
    "evaluate_cross_validation(clf_5b, X_train, y_train, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf_5c, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf_5d, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf_5e, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf_5f, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underway...\n",
      "[ 0.76618323  0.68694729  0.7336399   0.112133    0.79617834]\n",
      "Mean score: 0.619 (+/-0.128)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_5g, X_train, y_train, 5)\n",
    "#20 best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underway...\n",
      "[ 0.83763707  0.84294305  0.83409975  0.84931022  0.83581033]\n",
      "Mean score: 0.840 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_5h, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve the classification.  Change the max number of features, the smoothing (alpha) parameter on the MultinomialNB classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "clf_7 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                max_features=None, #50000,\n",
    "                stop_words='english',\n",
    "                token_pattern=\"[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\", \n",
    "#                 max_df=0.5,\n",
    "#                 sublinear_tf=True,\n",
    "#                 token_pattern=\"[a-z0-9_\\-\\.]+\",         \n",
    "    )),\n",
    "#     ('percSelect', SelectPercentile(f_classif, 0.25)),\n",
    "    ('clf', MultinomialNB(alpha=0.01)),\n",
    "#     ('clf', RandomForestClassifier()),\n",
    "#     ('clf', MLPClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cross_validation(clf_7, X_train, y_train, 5)\n",
    "# evaluate_cross_validation(clf_7, stemmed_data, news.target, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could continue doing trials by using different values of alpha or doing new modifications of the vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide that we have made enough improvements in our model, we are ready to evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print (\"Accuracy on training set:\")\n",
    "    print (clf.score(X_train, y_train))\n",
    "    print (\"Accuracy on testing set:\")\n",
    "    print (clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print (\"Classification Report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(clf_7, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we obtained very good results, and as we would expect, the accuracy in the training set is quite better than in the testing set. We may expect, in new unseen instances, an accuracy of around 0.91.\n",
    "\n",
    "If we look inside the vectorizer, we can see which tokens have been used to create our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_7.named_steps['vect'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_7.named_steps['vect'].get_stop_words())\n",
    "print(len(clf_7.named_steps['vect'].get_stop_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clf_7.named_steps['vect'].get_feature_names()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
