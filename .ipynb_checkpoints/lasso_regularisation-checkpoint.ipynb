{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation (Lasso)\n",
    "## Cambridge ML Commando Course\n",
    "\n",
    "In this notebook you should **(by completing bits of code left as \"...\")**:\n",
    "- create noisy data based on a pure signal\n",
    "- create regressors with various non-linear features\n",
    "- test their fits and plot them, along with printing their cross-validation scores\n",
    "- implement lasso regression to smooth out overfit, inspect how it works\n",
    "\n",
    "One you've done that, you can (no code needed):\n",
    "- plot a validation curve for our lasso regressor\n",
    "- plot learning curves for all our regressors\n",
    "- Check the correlation matrix of the data for clues about performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pure = np.arange(0,2*3.1415,0.1)\n",
    "\n",
    "true_fun = lambda x : np.sin(x) # Why not try a different function?\n",
    "\n",
    "np.random.seed(666)\n",
    "X = np.sort(random.choice(X_pure, size=30, replace=False))\n",
    "\n",
    "y_pure = np.sin(X_pure)\n",
    "y = true_fun(X) + np.random.randn(len(X))*0.25\n",
    "print(\"X values:\\n\",X)\n",
    "print(\"y values:\\n\",y)\n",
    "plt.plot(X_pure, true_fun(X_pure))\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "X_pure = X_pure.reshape(-1,1)\n",
    "\n",
    "plt.ylim(-1.6, 1.5)\n",
    "plt.plot(X_pure,true_fun(X_pure), linestyle=\"--\", label=\"true\")\n",
    "\n",
    "\n",
    "scaler15 = StandardScaler()\n",
    "poly15 = PolynomialFeatures(15)\n",
    "steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (\"reg\",LinearRegression())\n",
    "]\n",
    "reg15 = Pipeline( steps )\n",
    "\n",
    "reg15.fit(X,y)\n",
    "plt.plot(X_pure, reg15.predict(X_pure), label=\"quindecic (15)\")\n",
    "scores15 = cross_val_score(reg15, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "lasso_regressor = ... #Create your estimator here.\n",
    "\n",
    "steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (...) # put your lasso regressor here\n",
    "]\n",
    "lasso15 = Pipeline( steps )\n",
    "lasso15.fit(X,y)\n",
    "plt.plot(X_pure, lasso15.predict(X_pure), label=\"lasso (15)\")\n",
    "lasso_scores = cross_val_score(lasso15, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "print(\"Quindecic model\")\n",
    "print(-np.mean(scores15), np.std(scores15))\n",
    "\n",
    "print(\"Lasso regression regularisation\")\n",
    "print(-np.mean(lasso_scores), np.std(lasso_scores))\n",
    "\n",
    "plt.gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "How did that work?  Was it what you were expecting to see?\n",
    "Maybe it's time to investigate what effects changing *alpha* with have...\n",
    "\n",
    "PS: If you are getting convergence warnings, you may need to increase the Lasso object's tolerance (tol) to some larger value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = ... # implement a sequence over a log or linear space - you will probably need quite small values!\n",
    "ax = plt.figure().gca()\n",
    "ax.scatter(X, y)\n",
    "for a in alphas:\n",
    "    steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (...) # put your lasso regressor here - don't forget to set its \"alpha=\" keyword!\n",
    "    ]\n",
    "    new_lasso = Pipeline( steps )\n",
    "    new_lasso.fit(X,y)\n",
    "    ax.plot(X_pure, new_ridge.predict(X_pure), label=a)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.legend(title=\"alpha\")\n",
    "plt.gcf().set_size_inches(10,10)\n",
    "plt.title('Lasso (15): Fit to datapoints under increasing regularisation')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for a in alphas: # you can use the alphas you defined earlier\n",
    "    steps = [\n",
    "    (\"poly\",poly15),\n",
    "    (\"scale\",scaler15),\n",
    "    (...) # lasso regressor here\n",
    "    ]\n",
    "    new_lasso = Pipeline( steps )\n",
    "    new_lasso.fit(X,y)    \n",
    "    coefs.append(new_lasso.named_steps[\"reg\"].coef_)\n",
    "\n",
    "plt.gcf().set_size_inches(10,10)\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularisation param')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see how coefficients are being dropped to zero as you increase alpha?  This removes the corresponding features from the model, making it *sparse*.\n",
    "\n",
    "Have a look at some more characteristics of the lasso regressor below, using validation and learning curves.  You dont need to fill in any of this code, but you might want to take the best value and apply it to your code above, and see if it improves the fit and the cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "def plot_validation_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, param_name=\"C\", param_range = np.logspace(-3, 5, 10)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_scores, test_scores = validation_curve(\n",
    "    estimator, X, y, param_name=param_name, scoring=\"neg_mean_squared_error\", param_range=param_range,\n",
    "    cv=cv, n_jobs=n_jobs)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "            \n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "    lw = 2\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "\n",
    "    plt.gcf().set_size_inches(10,10)\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X15=scaler15.transform(poly15.transform(X)) # here we have to transform the X values ourselves, without relying on a pipeline\n",
    "plot_validation_curve(Lasso(tol=0.05), \"Lasso15\", X15, y, (-1.5,0.5), cv=10, n_jobs=-1, param_name=\"alpha\", param_range=np.logspace(-4, 1, 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, scoring=\"neg_mean_squared_error\", train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     print(test_scores_mean)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", alpha=0.5,\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", alpha=0.5,\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.gcf().set_size_inches(10,10)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n",
    "\n",
    "# train_sizes = np.linspace(1,15,15).astype(int)\n",
    "train_sizes = np.linspace(0.1, 1.0, 20)\n",
    "print(\"Using following proportions of data:\",train_sizes)\n",
    "plot_learning_curve(reg15, \"Overfit (15)\", X, y, (-.2e8,.5e7), cv=cv, n_jobs=4, train_sizes=train_sizes)\n",
    "plot_learning_curve(lasso15, \"Lasso (15)\", X, y, (-4,1), cv=cv, n_jobs=4, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End note\n",
    "It seems like the lasso regressor struggles a bit with this data.  You need to set the *alpha* value very small to avoid overly sparse regularisation, which leads to underfit.  This in turn means we need to increase the tolerance on the estimator, otherwise it does not converge properly.\n",
    "\n",
    "Can you think why this might be?  Have a look at the correlation matrix (below) for clues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.corrcoef(X15[:,1:].T) # remove first feature (always zero) and transpose (one feature per row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
