{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dimensionality Reduction with PCA\n",
    "Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "remove_setosa = False\n",
    "\n",
    "if remove_setosa:\n",
    "    indices_no_setosa = (iris.target != 0) #[TRUE for non-setosa else FALSE]\n",
    "    X_iris = iris.data[indices_no_setosa]\n",
    "    y_iris = iris.target[indices_no_setosa]\n",
    "else:\n",
    "    X_iris = iris.data\n",
    "    y_iris = iris.target\n",
    "\n",
    "numpy.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Test set will be the 25% taken randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.10, random_state=33)\n",
    "\n",
    "colormarkers = [ ['red','s'], ['greenyellow','o'], ['blue','x']]\n",
    "\n",
    "# Scale the features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px,py = X_train[:,0], X_train[:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.figure().gca()\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "ax.scatter(px,py, alpha=0.33, c=y_train)\n",
    "ax.set_xlabel('P.length')\n",
    "ax.set_ylabel('S.length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_PCA = PCA(n_components=2)\n",
    "txd = clf_PCA.fit_transform(X_train[:,[0,2]])\n",
    "\n",
    "def draw_vector(j, v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "    ax.annotate(\"PC{}\".format(1+j), v1)\n",
    "\n",
    "# plot data\n",
    "px,py = X_train[:,0],X_train[:,2] \n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(px,py, alpha=0.2, c=y_train)\n",
    "print(\"Our PCA scale factors and direction vectors are:\")\n",
    "for i, (length, vector) in enumerate(zip(clf_PCA.explained_variance_, clf_PCA.components_)):\n",
    "    print(length, vector)\n",
    "    v = vector * np.sqrt(length)\n",
    "#     print(v)\n",
    "    draw_vector(i, clf_PCA.mean_, clf_PCA.mean_ + v)\n",
    "plt.show()\n",
    "\n",
    "print(\"Replot, with rotated X,y values,\")\n",
    "print(\"And project points onto PC1:\")\n",
    "px, py = txd[:,0], txd[:,1]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "plt.axis(\"equal\")\n",
    "ax.scatter(px,py, alpha=0.33, c=y_train)\n",
    "ax.scatter(px,[0 for p in px], alpha=0.7, c=y_train)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "for i,xv in enumerate(clf_PCA.explained_variance_ratio_):\n",
    "    print(\"PC{} explains {:.2f}%\".format(1+i,xv*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition of the Covariance Matrix\n",
    "Underneath the sklearn wrapper, PCA is doing SVD on the Covariance Matrix of our features.  Here we do the same steps ourselves.  Compare the U,S,V values to the scale and direction values in the section above.  They should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train[:,[0,2]]\n",
    "print(M.shape)\n",
    "\n",
    "covariance_mx = numpy.cov(M.T)\n",
    "print(\"Numpy cov:\\n\",covariance_mx)\n",
    "\n",
    "# Do not confuse the covariance matrix with the correlation matrix!\n",
    "# corr_mx = numpy.corrcoef(M.T)\n",
    "# print(\"Numpy corr:\\n\",corr_mx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(covariance_mx)\n",
    "\n",
    "# # We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(covariance_mx)))\n",
    "ax.set_yticks(np.arange(len(covariance_mx)))\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(covariance_mx.shape[0]):\n",
    "    for j in range(covariance_mx.shape[0]):\n",
    "        text = ax.text(j, i, round(covariance_mx[i, j],3),\n",
    "                       ha=\"center\", va=\"center\", color=\"grey\")\n",
    "\n",
    "ax.set_title(\"Covariance matrix\")\n",
    "# fig.tight_layout()\n",
    "fig.set_size_inches(3,3)\n",
    "plt.show()\n",
    "\n",
    "U,S,V = sp.linalg.svd(covariance_mx)\n",
    "print(\"U matrix (PC directions)\")\n",
    "print(U)\n",
    "print(\"\\nS(igma) matrix (scale values)\")\n",
    "print(S)\n",
    "print(\"\\nV matrix (same as U)\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree plot\n",
    "Now we compress the full set of Iris data (4 components) using PCA.\n",
    "We use the output to create a scree plot which can be used to select an appropriate dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_PCA = PCA(n_components=4)\n",
    "clf_PCA.fit(X_train)\n",
    "\n",
    "cmps = clf_PCA.components_\n",
    "print(cmps)\n",
    "\n",
    "variance = clf_PCA.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "var=np.cumsum(np.round(clf_PCA.explained_variance_ratio_, decimals=3)*100)\n",
    "var #cumulative sum of variance explained with [n] features\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Components')\n",
    "plt.title('Iris data, PCA Scree Plot')\n",
    "plt.ylim(70,100.5)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.plot([1,2,3,4],var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher dimensioned datasets\n",
    "Now we do some PCA and scree plots with the wine (13D) and Boston (14D if we include price) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = sklearn.datasets.load_wine()\n",
    "# X = sklearn.preprocessing.scale(wine.data)\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "k = X.shape[1]\n",
    "clf_PCA = PCA(n_components=k)\n",
    "txd = clf_PCA.fit_transform(X)\n",
    "\n",
    "plt.scatter(txd[:,0], txd[:,1], c=y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Wine, reduced from 13D\")\n",
    "plt.show()\n",
    "variance = clf_PCA.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(clf_PCA.explained_variance_ratio_, decimals=3)*100)\n",
    "ax = plt.figure().gca()\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Components')\n",
    "plt.title('Wine Scree Plot')\n",
    "plt.ylim(0,100.5)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.plot(list(range(k)),var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = sklearn.datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "mms = sklearn.preprocessing.MinMaxScaler()\n",
    "X = mms.fit_transform(numpy.c_[X,y])\n",
    "# X = mms.fit_transform(X)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "k = X.shape[1]\n",
    "clf_PCA = PCA(n_components=k)\n",
    "txd = clf_PCA.fit_transform(X)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(txd[:,0], txd[:,1], c=y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Boston, reduced from 14D\")\n",
    "plt.show()\n",
    "variance = clf_PCA.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(clf_PCA.explained_variance_ratio_, decimals=3)*100)\n",
    "ax = plt.figure().gca()\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Components')\n",
    "plt.title('Boston Scree Plot')\n",
    "plt.ylim(0,100.5)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.plot(list(range(k)),var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
