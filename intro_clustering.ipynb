{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Test set will be the 25% taken randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.25, random_state=33)\n",
    "    \n",
    "# Standarise the features\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.figure()\n",
    "#     plt.clf()\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_train)\n",
    "# X_2d = pca_2d.fit_transform(X_train[:,[0,1]])\n",
    "# X_2d = X_train[:,[0,1]]\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "plt.title('Unmarked datapoints')\n",
    "# plt.xlim(sl.min(), sl.max())\n",
    "# plt.ylim(sw.min(), sw.max())\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormarkers = [ ['red','s'], ['greenyellow','o'], ['blue','x']]\n",
    "\n",
    "Zs = None\n",
    "#make a grid of numbers that span from min:max of sepal length and width\n",
    "def make_bg_grid(clf, X_train):\n",
    "    step = .01 \n",
    "    margin = .1   \n",
    "    sl_min, sl_max = X_train[:, 0].min()-margin, X_train[:, 0].max() + margin\n",
    "    sw_min, sw_max = X_train[:, 1].min()-margin, X_train[:, 1].max() + margin\n",
    "    #use numpy meshgrid to make a rectangular array from the limit values\n",
    "    sl, sw  = np.meshgrid(\n",
    "        np.arange(sl_min, sl_max, step), #arange creates an array that spans the specified range\n",
    "        np.arange(sw_min, sw_max, step)\n",
    "        )\n",
    "    try:\n",
    "        Zs = clf.predict( pca_2d.inverse_transform(np.c_[sl.ravel(), sw.ravel()])).reshape(sl.shape) # predict a Z value for each point in the grid\n",
    "    except:\n",
    "        print(\"Problem creating coloured backdrop\")\n",
    "        Zs = None\n",
    "    return Zs, sl, sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "clf_all = cluster.KMeans(init='k-means++', n_clusters=3, random_state=666)\n",
    "clf_all.fit(X_train)\n",
    "\n",
    "for clf in [clf_all]:\n",
    "    Zs, sl, sw = make_bg_grid(clf, X_2d)\n",
    "    centroids_s = clf.cluster_centers_\n",
    "    try:\n",
    "        tx_centroids = pca_2d.transform(centroids_s)\n",
    "    except:\n",
    "        tx_centroids = centroids_s\n",
    "    \n",
    "    #rjm49 - Now we've got the centroids, we can display the data points and the calculated regions, add the centroids as an extra scatter\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.clf()\n",
    "    if Zs is not None:\n",
    "        plt.imshow(Zs, interpolation='nearest', extent=(sl.min(), sl.max(), sw.min(), sw.max()), cmap= plt.cm.Pastel1, aspect='auto', origin='lower')\n",
    "    for j in [0,1,2]:\n",
    "#         px = X_train[:, 0][y_train == j]\n",
    "#         py = X_train[:, 1][y_train == j]\n",
    "        px = X_2d[:, 0][y_train == j]\n",
    "        py = X_2d[:, 1][y_train == j]\n",
    "\n",
    "        plt.scatter(px, py, c=colormarkers[j][0], marker= colormarkers[j][1])\n",
    "    plt.scatter(tx_centroids[:, 0], tx_centroids[:, 1],marker='*',linewidths=3, color='black', zorder=10)\n",
    "    plt.title('K-means clustering on the Iris dataset using all dimensions PCA\\'d\\nCentroids are marked with stars')\n",
    "    plt.xlim(sl.min(), sl.max())\n",
    "    plt.ylim(sw.min(), sw.max())\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we grow the number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clus in [2,3,5,10]:\n",
    "    clf = cluster.KMeans(init='k-means++', n_clusters=n_clus, random_state=33)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "#     Zs = clf.predict( pca_2d.inverse_transform(np.c_[sl.ravel(), sw.ravel()])).reshape(sl.shape) # predict a Z value for each point in the grid\n",
    "    Zs, sl, sw = make_bg_grid(clf, X_2d)\n",
    "#     plt.figure()\n",
    "    plt.imshow(Zs, interpolation='nearest', extent=(sl.min(), sl.max(), sw.min(), sw.max()), cmap= plt.cm.Pastel1, aspect='auto', origin='lower')\n",
    "    for j in [0,1,2]:\n",
    "        px = X_2d[:, 0][y_train == j]\n",
    "        py = X_2d[:, 1][y_train == j]\n",
    "        plt.scatter(px, py, c=colormarkers[j][0], marker= colormarkers[j][1])\n",
    "#         plt.scatter(px,py, c=plt.cm.Dark2.colors[j])\n",
    "    centroids_s = clf.cluster_centers_\n",
    "    centroids_s = pca_2d.transform(centroids_s)\n",
    "    plt.scatter(centroids_s[:, 0], centroids_s[:, 1],marker='*',linewidths=3, color='black', zorder=10)\n",
    "    plt.title('K-means clustering on the Iris dataset\\nAll dims, PCA\\'d, k={}\\nCentroids are marked with stars'.format(n_clus))\n",
    "    plt.xlim(sl.min(), sl.max())\n",
    "    plt.ylim(sw.min(), sw.max())\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For higher dim datasets, may be preferable to do PCA first then scree plot\n",
    "# pca = PCA(n_components=4)\n",
    "# X_scree = pca.fit_transform(X_train)\n",
    "X_scree = X_train\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = cluster.KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(X_scree)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('Number of clusters, k')\n",
    "plt.ylabel('Model inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Bad Clustering Chart\n",
    "Many types of clustering are available, all with strengths and weaknesses.  For example Affinity Propagation does not require you to specify a number of clusters (but has other parameters instead) and Spectral Clustering can pick out concave and interlocking shapes (whereas K-means just finds blobs).  However they are more complicated to train and use (see the training times given below).\n",
    "\n",
    "This chart gives some of the main ones and shows them in use on several synthetic datasets with various characteristics.\n",
    "\n",
    "Much more info about clustering can be found at:\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2,\n",
    "                     'min_samples': 20, 'xi': 0.25}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "# Experimental OPTICS class\n",
    "#     optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
    "#                             xi=params['xi'],\n",
    "#                             min_cluster_size=params['min_cluster_size'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(random_state=666,\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ag.C Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "#         ('OPTICS', optics),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We used k-means clustering to group our flower data into sub-categories (without using the labels).\n",
    "- We got to grips with some more complicated graph plotting code\n",
    "- We varied the number of clusters to see how we can force k-means to find sub-categories\n",
    "- We used a scree plot of model inertia to estimate a good value for k\n",
    "- We checked out the Big Bad Clustering Chart for alternative clustering techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
