{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M/L Commando Course\n",
    "## Multi* classification\n",
    "Scikit learn (v0.24) has: \n",
    "* good support for multiclass problems (now natively supported by all classifiers)\n",
    "* OK support for multilabel problems\n",
    "* Spotty support for multioutput-multiclass problems (for instance, no metric/scoring support at all!)\n",
    "\n",
    "In this notebook we demonstrate the first of these and try to deal with the latter two...\n",
    "\n",
    "Our current recommendation is to use a different tech for the latter two, such as Keras (neural networks) ... this gives you much more control over how you apply loss functions and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 0.23.1\n",
      "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from random import random, shuffle, choice, randint\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "print(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass - MNIST digit recognition\n",
    "A multiclass problem is simply a non-binary classification problem.\n",
    "\n",
    "Typically $y = n$ where $n \\in {Class~IDs}$\n",
    "\n",
    "Alternative one-hot encoding can be used:\n",
    "\n",
    "$y_{one~hot}=[0,0, \\cdots, y_{C}=1 , \\cdots ,0,0]$ which indicates $y \\in C$\n",
    "\n",
    "In a regular multiclass problem, each sample can be a member of only one class.  (If this is not the case, consider a multi-label approach instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafname = \"data/mnist_data.npz\"\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "# mnist_data = mnist.load_data()\n",
    "# np.savez_compressed(datafname, np.array(mnist_data))\n",
    "mnist_data = np.load(datafname)[\"arr_0\"]\n",
    "((tX, ty), (vX, vy)) = mnist_data\n",
    "\n",
    "print(mnist_data)\n",
    "\n",
    "print(tX.shape)\n",
    "print(ty.shape, \"<- cardinality=1\")\n",
    "print(np.unique(ty), \"<- multiclass classes=10\")\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(330+1+i)\n",
    "    plt.imshow(tX[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tX = tX.reshape((len(tX), 28*28))\n",
    "print(tX.shape)\n",
    "vX = vX.reshape((len(vX), 28*28))\n",
    "print(ty.shape)\n",
    "\n",
    "max_n = 1000\n",
    "\n",
    "print(np.min(tX), np.max(tX))\n",
    "sc = MinMaxScaler()\n",
    "# tX = sc.fit_transform(tX)[0:max_n, :]\n",
    "# vX = sc.transform(vX)[0:max_n,:]\n",
    "print(np.min(tX), np.max(tX))\n",
    "ty = ty[0:max_n]\n",
    "vy = vy[0:max_n]\n",
    "print(tX.shape)\n",
    "print(ty.shape)\n",
    "\n",
    "# for i in range(9):\n",
    "#     plt.subplot(330+1+i)\n",
    "#     plt.imshow(tX[i].reshape(28,28))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, f1_score\n",
    "\n",
    "estimators = [LogisticRegression(multi_class=\"multinomial\", max_iter=1000), \n",
    "              LogisticRegression(multi_class=\"ovr\", max_iter=1000),\n",
    "              SGDClassifier(max_iter=1000),\n",
    "              SVC(),\n",
    "             ]\n",
    "for ix,est in enumerate(estimators):\n",
    "    est.fit(tX,ty)\n",
    "    print(\"Estimator #{}\".format(ix))\n",
    "    scs = cross_val_score(est, tX, ty, cv=5, scoring=\"f1_weighted\")\n",
    "    print(scs, np.mean(scs))\n",
    "#     print(confusion_matrix(ty, est.predict(tX)))\n",
    "    print(\"Test set f1\", f1_score(vy, est.predict(vX), average=\"weighted\"))\n",
    "    print(\"Test set confusion mx:\\n\",confusion_matrix(vy, est.predict(vX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel\n",
    "Multioutput-multilabel (or multilabel-indicator) problems have a \"wide\" target, where each element can take a binary value (any number of these can be 1).\n",
    "\n",
    "$y = [y_0, y_1, y_2 ... y_N]$ where $y_0 \\in \\{0,1\\}$, $y1 \\in \\{0,1\\}$, etc\n",
    "\n",
    "The binary flags show membership of each class, so column 0 represents $Class_0$, column 1 is $Class_1$, etc\n",
    "\n",
    "It may be easier not to think of these as \"classes\" at all but tags (or labels as the name suggests) which are non mutually exclusive:  e.g. films might have (comedy)+(horror), (comedy)+(romance), (action)+(horror) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP [0.73613656 0.70431199 0.72587237 0.73444921 0.73579433] 0.7273128942031183\n",
      "[[[473   0]\n",
      "  [  0 277]]\n",
      "\n",
      " [[458   0]\n",
      "  [  0 292]]\n",
      "\n",
      " [[485   0]\n",
      "  [  0 265]]\n",
      "\n",
      " [[457   0]\n",
      "  [  0 293]]\n",
      "\n",
      " [[420   0]\n",
      "  [  0 330]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, SGDRegressor, LogisticRegression, RidgeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=5, n_labels=2,\n",
    "                                      allow_unlabeled=True,\n",
    "                                      random_state=666)\n",
    "\n",
    "print(\"\\nType of target is\", type_of_target(y),\"\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "print(\"Try normal classifier\")\n",
    "est = LogisticRegression() #SGDClassifier(loss=\"log\")\n",
    "try:\n",
    "    est.fit(X_train, y_train)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "this_scoring = \"f1_weighted\"\n",
    "est= MultiOutputClassifier(LogisticRegression())\n",
    "est.fit(X_train, y_train)\n",
    "scs = cross_val_score(est, X_train, y_train, cv=5, scoring=this_scoring)\n",
    "print(\"\\nMOC\", scs, np.mean(scs))\n",
    "print(multilabel_confusion_matrix(y_train, est.predict(X_train)))\n",
    "    \n",
    "est = ClassifierChain(LogisticRegression())\n",
    "print(y_train.shape, y_train.ndim)\n",
    "est.fit(X_train, y_train)\n",
    "scs = cross_val_score(est, X_train, y_train, cv=5, scoring=this_scoring)\n",
    "print(\"\\nCCh\", scs, np.mean(scs))\n",
    "print(multilabel_confusion_matrix(y_train, est.predict(X_train)))\n",
    "\n",
    "est= OneVsRestClassifier(LogisticRegression())\n",
    "est.fit(X_train, y_train)\n",
    "scs = cross_val_score(est, X_train, y_train, cv=5, scoring=this_scoring)\n",
    "print(\"\\nOvR\", scs, np.mean(scs))\n",
    "print(multilabel_confusion_matrix(y_train, est.predict(X_train)))\n",
    "\n",
    "#Multi-layer perceptron (Neural Network) has native multilabel support\n",
    "est= MLPClassifier(max_iter=10000, early_stopping=False)\n",
    "est.fit(X_train, y_train)\n",
    "scs = cross_val_score(est, X_train, y_train, cv=5, scoring=this_scoring)\n",
    "print(\"\\nMLP\", scs, np.mean(scs))\n",
    "print(multilabel_confusion_matrix(y_train, est.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass-Multioutput\n",
    "Multiclass-multioutput problems have a \"wide\" target, where each element can take a class value (these do not have to be binary).\n",
    "\n",
    "We have N sets of possible \"class groups\" $G_0, G_1 .. G_N$\n",
    "Each class group is some set of classes $G_0 = \\{C_{0a}, C_{0b}, ..\\}$; $G_1=\\{C_{1a}, C_{1b}, ...\\}$ \n",
    "\n",
    "$y = [y_0, y_1, y_2 ... y_N]$ where $y_0 \\in G_0$, $y1 \\in G_1$, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=None\n",
    "n_classes_per_col = 3 # in practice there might a different n of classes for each column\n",
    "y_width = 3  # this is the total width of each y entry\n",
    "X_width = 10 # this is the desired total width of each X entry (total num of features)\n",
    "av_labels_per_multilabel_iter=2 # controls the density of binary labels per accumulative iteration\n",
    "features_per_iter= int(X_width/(n_classes_per_col-1))\n",
    "for seed in range(n_classes_per_col-1):\n",
    "    tempX, tempy = make_multilabel_classification(n_samples=1000, \n",
    "                                        n_features= features_per_iter, \n",
    "                                        n_classes=y_width, \n",
    "                                        n_labels=av_labels_per_multilabel_iter,\n",
    "                                        allow_unlabeled=True,\n",
    "                                        random_state=seed)\n",
    "    if X is None:\n",
    "        X=tempX\n",
    "        y=tempy\n",
    "    else:\n",
    "        X = np.concatenate([X,tempX], axis=1)\n",
    "        y += tempy\n",
    "        \n",
    "print(X.shape, y.shape)\n",
    "print(\"X training sample\\n\", X[0:5])\n",
    "print(\"y training sample\\n\", y[0:5])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "print(\"Type of target=\", type_of_target(y_train))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "estimators= [ MultiOutputClassifier(LogisticRegression(multi_class=\"multinomial\")), KNeighborsClassifier() ]\n",
    "\n",
    "# Have to do a manual evaluation, since sklearn's metrics don't support multioutput-multiclass yet...\n",
    "for est in estimators:\n",
    "    print(\"\\n\", type(est).__name__)\n",
    "    est.fit(X_train, y_train)\n",
    "    y_hats = est.predict(X_test)\n",
    "    f1s = []\n",
    "    for c in range(y_test.shape[1]):\n",
    "        yc = y_test[:,c]\n",
    "        yhc = y_hats[:,c]\n",
    "        this_f1 = f1_score(yc, yhc, average=\"weighted\")\n",
    "        f1s.append(this_f1)\n",
    "        print(\"For column {} F1 = {}\".format(c, this_f1))\n",
    "        print(confusion_matrix(yc,yhc))\n",
    "    print(\"Per column F1s:\", np.round(f1s,2), \"Mean F1:\", np.round(np.mean(f1s),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
