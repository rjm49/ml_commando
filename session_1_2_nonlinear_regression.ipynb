{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1.2- Non-linear (Polynomial) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "from sklearn import preprocessing\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dataset using our coding skillz\n",
    "First we will create a dataset.  This time we won't rely on sklearn since it does not appear to support basic non-linear regression problems.  It is easy to make our own, we will use a sine wave to make a curvy output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = False\n",
    "if gen_data:\n",
    "    # X_pure = np.arange(0,2*math.pi,0.01) # <-- \"array-range\" (start_val, stop_val, increment_value)\n",
    "    # print(len(X_pure))\n",
    "    X_pure = np.linspace(0,2*math.pi, 1000) # <-- just a nicer way to do it: (start_val, stop_val, num_increments)\n",
    "    print(len(X_pure))\n",
    "    true_fun = lambda x : 10*np.sin(x)\n",
    "\n",
    "    np.random.seed(666)\n",
    "    X = np.sort(random.choice(X_pure, size=100, replace=False))\n",
    "\n",
    "    y_pure = np.sin(X_pure)\n",
    "    y = true_fun(X) + np.random.randn(len(X)) # generate points with Gaussian noise\n",
    "    plt.plot(X_pure, true_fun(X_pure), linestyle=\"-.\")\n",
    "else:\n",
    "    #Alternative dataset, hours vs 1000s_of_bacteria, exponential growth y=Ae^(ax)\n",
    "    X_pure = None\n",
    "    data = [(2.5, 10.07),\n",
    "    (2.8, 11.07),\n",
    "    (5.4, 14.59),\n",
    "    (6.5, 20.70),\n",
    "    (9.2, 27.94),\n",
    "    (9.5, 31.50),\n",
    "    (11.0, 38.04),\n",
    "    (13.3, 49.90),\n",
    "    (14.6, 60.72),\n",
    "    (16.4, 75.57)]\n",
    "    X = np.array([tup[0] for tup in data]).reshape(-1,1)\n",
    "    y = np.array([tup[1] for tup in data]).ravel()\n",
    "    \n",
    "features_names = [\"The Feature\"]\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(features_names[0])\n",
    "\n",
    "X = X.reshape(-1,1) # need to convert X from a list to a list of single-feature-lists [[x1],[x2],[x3] ...]\n",
    "print(X.shape, y.shape)\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the dataset\n",
    "As before, manually partition the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(X)\n",
    "\n",
    "test_split = 0.25\n",
    "test_n = int(test_split*n)\n",
    "\n",
    "all_indices = list(np.arange(n))\n",
    "numpy.random.seed(666)\n",
    "test_indices = numpy.random.choice(all_indices, size=test_n, replace=False)\n",
    "train_indices = list(set(all_indices) - set(test_indices))\n",
    "\n",
    "X_train  = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# convenience class to do the above...\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=666)\n",
    "\n",
    "if X_pure is not None:\n",
    "    plt.plot(X_pure, true_fun(X_pure), linestyle=\"--\", label=\"True\")\n",
    "plt.scatter(X_train, y_train, label=\"Training set\")\n",
    "plt.scatter(X_test, y_test, label=\"Test set\") #it's sneaky to look at your test data!\n",
    "plt.legend() # gets its labels from the plot and scatter \"label\" params\n",
    "plt.axhline(np.mean(y_train), color=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training wrapper\n",
    "This routine trains an estimator and carries out k-fold cross-validation to get a \"typical\" score for performance.\n",
    "\n",
    "We specify a \"refit\" parameter which is False by default.  The routine checks if the estimator is already fitted and if so, it does not train again.  It can be overridden if training is desired (set the param to True at method invocation).\n",
    "\n",
    "If the number of folds is set too low (i.e. <2) then the routine assumes you don't want cross validation at all, and will just score the estimator on the _X and _y values passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def train_and_evaluate(_reg, _X, _y, n_folds=0, refit=False):\n",
    "    # Estimator objects can be fitted or unfitted.  If the user wants to force a retraining, then we do not need to check the status of the estimator\n",
    "    # PART 1: Check whether we need to refit\n",
    "    if refit==True:\n",
    "        do_train=True\n",
    "    else:\n",
    "        try:\n",
    "            check_is_fitted(_reg)\n",
    "            print(\"Estimator is already fitted: will assume you are using the test set and not train it again!\")\n",
    "            do_train=False\n",
    "        except:\n",
    "            do_train=True\n",
    "\n",
    "    # PART 2: If we must refit, do the training here\n",
    "    if do_train:\n",
    "        _reg.fit(_X, _y)\n",
    "        print ('Score on training set: {:.2f}'.format(_reg.score(_X, _y)))\n",
    "\n",
    "    # PART 3: test for goodness of fit\n",
    "    if n_folds >= 2: # we need at least 2 folds to do cross validation\n",
    "        cv = sklearn.model_selection.KFold(n_splits= n_folds, shuffle=True, random_state=666)\n",
    "        scores = sklearn.model_selection.cross_val_score(_reg, _X, _y, cv=cv, scoring=\"explained_variance\")\n",
    "        av_score = np.mean(scores)\n",
    "        print ('Average score using {}-fold crossvalidation:{:.2f}'.format(n_folds,av_score))\n",
    "    else: #otherwise just do a normal scoring\n",
    "        av_score = sklearn.metrics.explained_variance_score(_reg.predict(_X),_y)\n",
    "        print(\"Plain (non-CV) score on X vs y:{:.2f}\".format(av_score))\n",
    "    return _reg, av_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new F-test routine\n",
    "Define the F-test function, slighty funkier than before because we need to deal with models of varying complexity (i.e. different numbers of model parameters)\n",
    "\n",
    "This routine in particular is modelled on [this specification](https://sites.duke.edu/bossbackup/files/2013/02/FTestTutorial.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def f_test(residuals_1, residuals_2, nparams_1=2, nparams_2=2, alpha=0.95):\n",
    "    SSR1 = numpy.sum(numpy.power(residuals_1,2))\n",
    "    SSR2 = numpy.sum(numpy.power(residuals_2,2))\n",
    "    print(SSR1, SSR2)\n",
    "    \n",
    "    dfn = len(residuals_1)-nparams_1 #define degrees of freedom numerator \n",
    "    dfd = len(residuals_2)-nparams_2 #define degrees of freedom denominator \n",
    "    print(\"dfs:\", dfn, dfd)\n",
    "\n",
    "    if nparams_1 == nparams_2:\n",
    "        f = (SSR1/SSR2)\n",
    "    else:\n",
    "        if nparams_1 < nparams_2: # model 1 must be simpler for this formula\n",
    "            f = ((SSR1/SSR2)/(dfn-dfd)) / (SSR2/dfd)\n",
    "        else:\n",
    "            raise Exception(\"Model params out of order: simpler model must come first\")\n",
    "    \n",
    "    p = 1-sp.stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic \n",
    "    # p is smaller when f is smaller\n",
    "    print(\"f is\",f)\n",
    "    print(\"p is\",p)\n",
    "    \n",
    "    # slightly different way of processing results if n_params is the same for both models\n",
    "    if nparams_1 != nparams_2:\n",
    "    #     In this case a p-value less than α indicates that the more complex model (denominator of F-statistic)\n",
    "    #     fits the data significantly better than the simpler model.        \n",
    "        if p < alpha: # if the probability of the null hypothesis is < the critical value\n",
    "            print(\"Model 2 is better\")\n",
    "        else:\n",
    "            print(\"F-test is inconclusive (cannot reject H0)\")    \n",
    "    else:\n",
    "    #     If the p-value is large (greater than α) then the first model is statistically better than the second.\n",
    "    #     If the p-value is small (less than 1-α) then the second model is statistically better than the first.\n",
    "        if p > alpha:\n",
    "            print(\"Model 1 is better\")\n",
    "        elif p < (1-alpha):\n",
    "            print(\"Model 2 is better\")\n",
    "        else:\n",
    "            print(\"F-test is inconclusive (cannot reject H0)\")\n",
    "    \n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear (polynomial) regression\n",
    "This time we will use a transformer object called PolynomialFeatures to bring in more complex non-linear features to the dataset.  The regression model remains linear, it is the features we are turning polynomial.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "polyfeats = PolynomialFeatures(degree=2)\n",
    "X_f = polyfeats.fit_transform(X_train)\n",
    "X_f_test = polyfeats.transform(X_test)\n",
    "\n",
    "est2 = LinearRegression() # common or garden OLS linear regression model .. the non-linearities are in the features\n",
    "est2.fit(X_f, y_train)\n",
    "\n",
    "for name, this_est in [(\"Quadratic reg.\", est2), ]:\n",
    "    print(name.upper())\n",
    "    for dataset_name, this_X, this_y in [(\"Training set\", X_f, y_train), (\"Test set\", X_f_test, y_test)]:\n",
    "        print(dataset_name)\n",
    "#         print(\"R2 is\", this_est.score(this_X, this_y))\n",
    "        # Don't use R2! https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/\n",
    "        this_y_hats = this_est.predict(this_X)\n",
    "#         av_score = sklearn.metrics.explained_variance_score(this_est.predict(this_X), this_y)\n",
    "#         print(\"Explained variance is {}\".format(av_score))\n",
    "        print(\"MAE is\", mean_absolute_error(this_y, this_y_hats))\n",
    "        print(\"MSE is\", mean_squared_error(this_y, this_y_hats))\n",
    "        print(\"RMSE is\", numpy.sqrt(mean_squared_error(this_y, this_y_hats)))\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features_param_search(_est, _X,_y, _X_test, _y_test):\n",
    "    #first plot the real dataset\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(_X, _y)\n",
    "    plt.scatter(_X_test, _y_test)\n",
    "    \n",
    "    # next prep some places to store results\n",
    "    best = (None, None, -math.inf, -math.inf)\n",
    "    y_hat_list = []\n",
    "    \n",
    "    # loop across a bunch of different polynomial degrees for increasingly complex curve fit\n",
    "    for deg in [1,2,3,7]:\n",
    "        for inter_only in [False, ]: #can switch interaction-only features on here if we ever want to (really for multivariate data)\n",
    "            print(\">>>\",deg)\n",
    "            polyfeat = PolynomialFeatures(degree=deg, interaction_only=inter_only)\n",
    "            X_f = polyfeat.fit_transform(_X)\n",
    "            _est, sc_tr = train_and_evaluate(_est, X_f, _y, 5, refit=True) # fit and cross val the estimator\n",
    "\n",
    "            y_hats = _est.predict(X_f)\n",
    "            y_hat_list.append((deg,y_hats)) #keep these for later!\n",
    "\n",
    "            # the next few lines are just to draw the regression line on the graph\n",
    "            xs = np.linspace(X_train.min()-1, X_train.max()+1,30).reshape(-1,1) \n",
    "            xs_poly = polyfeat.transform(xs)\n",
    "            y_hats_for_curve = _est.predict(xs_poly)\n",
    "            plt.ylim(min(_y)-1, max(_y)+1)\n",
    "            plt.plot(xs, y_hats_for_curve, label=str(deg)) # add the regression line for this run\n",
    "            \n",
    "            if sc_tr > best[2]: # if your cross-val'd score is best so far...\n",
    "                print(\"*** NEW BEST\")\n",
    "                X_f_test = polyfeat.transform(_X_test)\n",
    "                _est, sc_tt = train_and_evaluate(_est, X_f_test, _y_test, 0, refit=False) # get test data score\n",
    "                best = (deg, inter_only, sc_tr, sc_tt) # stash it all in a tuple\n",
    "    print(best)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return best, y_hat_list\n",
    "\n",
    "best, y_hat_list = poly_features_param_search(LinearRegression(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_residuals = lambda yh,y : (yh-y) # lambda expressions are a way of defining simple functions\n",
    "# def get_residuals(yh,y): return (yh-y) # same as above\n",
    "previous_residuals = None\n",
    "prev_deg = None\n",
    "f_test_results = []\n",
    "\n",
    "flat_ys = np.ones_like(y_train) * np.mean(y_train)\n",
    "if y_hat_list[0][0] != 0: # this stops us adding loads of mean-y estimators whenever we rerun this cell :)\n",
    "    y_hat_list.insert(0, (0, get_residuals(flat_ys, y_train)))\n",
    "\n",
    "for deg, y_hats in y_hat_list:\n",
    "    print(\">>>{}\".format(deg))\n",
    "    residuals = get_residuals(y_hats, y_train)\n",
    "#     print(residuals)\n",
    "    plt.scatter(X_train, residuals)\n",
    "    plt.axhline(0)\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(residuals)\n",
    "    plt.show()\n",
    "    \n",
    "    if previous_residuals is not None:\n",
    "        match_id = \"{} vs {}\".format(deg, prev_deg)\n",
    "        print(match_id)\n",
    "        f,p = f_test(previous_residuals, residuals, prev_deg, deg)\n",
    "        print(p)\n",
    "        f_test_results.append((match_id, p))\n",
    "    previous_residuals = residuals\n",
    "    prev_deg = deg\n",
    "    \n",
    "print(\"---\")\n",
    "for i,p in f_test_results:\n",
    "    print(i,p, \"Prob Different\" if p<0.05 else \"Cannot reject H0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- We generated a dataset based on a sine wave (or used bacterial growth data), and partitioned train/test sets\n",
    "- We implemented non-linear regression at the feature level using the PolynomialFeatures transformer\n",
    "- We set up a method to train and cross-validate our models\n",
    "- We did a parameter search across various degrees of polynomial to find a better fit\n",
    "- We used the F-test to decide which of our fits were significant improvements compared to degree (n-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
