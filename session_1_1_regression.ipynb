{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1.1 - Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "from sklearn import preprocessing\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "Linear regression is a very common and surprisingly useful technique for modelling the relationship between two or more variables or characteristics.  For instance, an idealised relationship between two values might take the form:\n",
    "\n",
    "$y = b + wx$\n",
    "\n",
    "Where $y$ is the dependent variable like (house price) or (crop yield), and $x$ is the independent or controlling variable like (distance from capital) or (amount of fertiliser).\n",
    "\n",
    "The relationship between these two variables is a line with slope $w$ and intercept $b$.\n",
    "\n",
    "In practice when we collect data from the real world, there is some noise in the $y$ values.  In linear regression what we aim to do is _fit_ a line to our datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dataset using scikit-learn's utils\n",
    "First we will create a dataset to try out linear regression.  Luckily scikit-learn has a module (sklearn.datasets) with both a nice selection of pre-built datasets, and various routines to generate datasets.  We can use [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) to synthesise a typical sort of regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "use_diabetes = False\n",
    "if use_diabetes:\n",
    "    data_bunch = load_diabetes()\n",
    "    X = data_bunch.data[:,2] # this is the BMI column\n",
    "    y = data_bunch.target # this is the blood sugar level\n",
    "else:\n",
    "    X, y = make_regression(n_samples=100, n_features=1, noise=30, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make_regression call produces numpy arrays, which are matrix or table-like structures.  We can check the shape using the code below.  Note that X is technically 2D, because it is a matrix of 50 rows, with one datapoint in each row (this is how sklearn \"thinks\" about data).  On the other hand, y for some reason is just a list of target values and so it is only 1D.  This is why X is in caps and y is lower case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X), type(y))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the dataset\n",
    "Next we have to split the dataset into training and test sets.  This is fairly normal - we do our machine learning on the training set, but to see how well we've done, we need to test it on some unseen data to make sure it's not \"overfitting\" to some peculiarities in the training set.\n",
    "\n",
    "In fact, to do this exactly right, we should never reuse the test set (or even look at it), because doing so introduces bias to our evaluation.  However in practice data is quite a limited resource so this rule is often rather loosely followed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "\n",
    "# numpy.random.seed(666)\n",
    "test_split = 0.25\n",
    "test_n = int(test_split*n)\n",
    "test_indices = numpy.random.randint(0,n, size=test_n)\n",
    "\n",
    "\n",
    "train_indices = list(set(range(n)) - set(test_indices))\n",
    "\n",
    "X_train  = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# convenience class to do the above...\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=666)\n",
    "\n",
    "# plt.figure(figsize=(10,10)) # set figure size to 10x10 inches\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.scatter(X_test, y_test) #it's sneaky to look at your test data!\n",
    "plt.legend(labels=[\"Training set\", \"Test set\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY Linear Regression\n",
    "Mostly we will be using scikit-learn's tools (and a few others) but let's start by rolling our own linear regression function.  Nothing fancy, it will just work for 2D (x,y) datapoints.  We will feed in the points and some configuration parameters.  The code performs gradient descent on the coefficients w and b.\n",
    "\n",
    "The routine returns a list of predicted y-values ($\\mathbf{\\hat{y}}$, _aka_ y_hats), as well as the resultant values for $w$ and $b$ and the final cost value (which is just the sum of the squared differences between y_hats and y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_linear_regression(xs,ys, n_iter=300, learning_rate_alpha=0.001):\n",
    "    \n",
    "    # define a local helper function to recalculate the y_hats and cost of the current line\n",
    "    def recalc_predictions(xs,ys, b,w):\n",
    "        cost = 0\n",
    "        y_hats = []\n",
    "        for x,y in zip(xs,ys):\n",
    "            y_hat = b+w*x\n",
    "            cost += (y_hat- y)**2\n",
    "            y_hats.append(y_hat)\n",
    "        return y_hats, cost\n",
    "    \n",
    "    N = len(xs)\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "\n",
    "    # this loop does the gradient descent proper\n",
    "    for i in range(n_iter):\n",
    "        y_hats, cost = recalc_predictions(xs,ys,b,w)\n",
    "#         if i % 100 == 0:\n",
    "#             print(\"cost at iteration {} = {}\".format(i,cost)) # purely for display purposes\n",
    "        change_w = 0.0\n",
    "        change_b = 0.0    \n",
    "        for x,y,y_hat in zip(xs,ys,y_hats):  \n",
    "            change_w +=  2*x * (y_hat - y) # get the gradient value for each point\n",
    "            change_b +=  2   * (y_hat - y) # bias doesn't depend on x, so the delta is a bit different\n",
    "        change_w = change_w/N # divide these to get an average\n",
    "        change_b = change_b/N\n",
    "        w = w - learning_rate_alpha * change_w # here we actually adjust the coefficients/weights!\n",
    "        b = b - learning_rate_alpha * change_b\n",
    "    return y_hats, w, b, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write our own estimator\n",
    "The _estimator_ type in Scikit-learn is one of the main classes.  It is used for both regression and classification tasks.  In this next section we will wrap the linear regression routine above into an estimator class called **MyLinearRegressor**.  This can then be used just like any estimator, via its **fit(X,y)** and **predict(X)** methods.\n",
    "\n",
    "You will not need to remember how to write an estimator for our workshops, but it might be useful in future - feel free to use this as starter code.\n",
    "\n",
    "For full details on writing estimators and other classes for scikit-learn, go here:\n",
    "https://scikit-learn.org/stable/developers/develop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import r2_score\n",
    "class MyLinearRegressor(BaseEstimator):\n",
    "    def __init__(self, demo_param='demo'):\n",
    "         self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        _,w,b,_ = my_linear_regression(X,y)\n",
    "        self.w_ =  w\n",
    "        self.b_ =  b\n",
    "         # Return the estimator\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        y_hats = self.b_ + self.w_*X\n",
    "        return y_hats\n",
    "\n",
    "    def score(self, X,y):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        y_hats = self.predict(X)\n",
    "        return r2_score(y, y_hats)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "\n",
    "X_train = X_train.reshape(-1,1) # you'll find yourself doing this a lot\n",
    "X_test = X_test.reshape(-1,1)\n",
    "\n",
    "# Let's create our regressor objects\n",
    "my_reg = MyLinearRegressor()\n",
    "reg = LinearRegression()\n",
    "sgd_reg = SGDRegressor(max_iter=10) # Stochastic Gradient Descent - can approximate LReg for very big problems\n",
    "\n",
    "# Train and predict\n",
    "my_reg.fit(X_train,y_train)\n",
    "diy_y_hats = my_reg.predict(X_train).ravel()\n",
    "\n",
    "reg.fit(X_train,y_train)\n",
    "y_hats = reg.predict(X_train).ravel()\n",
    "\n",
    "sgd_reg.fit(X_train, y_train)\n",
    "sgd_y_hats = sgd_reg.predict(X_train).ravel()\n",
    "\n",
    "# Use Matplotlib to plot dots and lines\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, diy_y_hats, color=\"green\", label=\"DIY\", alpha=0.5)\n",
    "plt.plot(X_train, sgd_y_hats, color=\"blue\", label=\"sk SGD\", alpha=0.5)\n",
    "plt.plot(X_train, y_hats, color=\"red\", label=\"sk OLSR\", alpha=0.5)\n",
    "plt.axhline(numpy.mean(y_train), color=\"gray\", linestyle=\"--\")\n",
    "plt.legend(labels=[\"DIY\",\"SGD\",\"OLSR\"])\n",
    "# plt.legend(labels=[\"Stochastic Grad. Desc.\", \"Ordinary Least Sqs.\"])\n",
    "plt.show()\n",
    "\n",
    "print(y_hats.shape)\n",
    "print(diy_y_hats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything you can change to improve our line?  \n",
    "Can you think of a different way to control the gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic measures of performance and goodness-of-fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "for name, this_reg in [(\"Ord.Least.Sqs\", reg), (\"Stoch.Grad.Desc.\", sgd_reg), (\"DIY.Grad.Desc\", my_reg)]:\n",
    "    print(name.upper())\n",
    "    for dataset_name, this_X, this_y in [(\"Training set\", X_train, y_train), (\"Test set\", X_test, y_test)]:\n",
    "        print(dataset_name)\n",
    "        print(\"R2 is\", this_reg.score(this_X, this_y))\n",
    "        this_y_hats = this_reg.predict(this_X)\n",
    "        print(\"MAE is\", mean_absolute_error(this_y, this_y_hats))\n",
    "        print(\"MSE is\", mean_squared_error(this_y, this_y_hats))\n",
    "        print(\"RMSE is\", numpy.sqrt(mean_squared_error(this_y, this_y_hats)))\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the residuals and do an F-Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_residuals = lambda yh,y : (yh-y) # lambda expressions are a way of defining simple functions\n",
    "\n",
    "diy_y_hats = numpy.array(diy_y_hats).ravel() # use ravel to flatten out the y_hat values\n",
    "\n",
    "diy_residuals = get_residuals(diy_y_hats, y_train)\n",
    "sk_residuals = get_residuals(y_hats, y_train)\n",
    "\n",
    "for title,residuals in [(\"DIY\", diy_residuals), (\"Scikit-learn OLSR\",sk_residuals)]:\n",
    "    plt.hist(residuals)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(X_train, residuals)\n",
    "    plt.axhline(0)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print(np.mean(residuals), np.std(residuals))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define F-test function\n",
    "def f_test(residuals_1, residuals_2, nparams=2, alpha=0.95):\n",
    "    SSR1 = numpy.sum(residuals_1**2)\n",
    "    SSR2 = numpy.sum(residuals_2**2)\n",
    "    print(\"Sum of Sqd residuals:\")\n",
    "    print(\"SSR1 =\",SSR1, \"SSR2 =\",SSR2)\n",
    "    f = (SSR1/SSR2)\n",
    "    \n",
    "    dfn = len(residuals_1)-nparams #define degrees of freedom numerator \n",
    "    dfd = len(residuals_2)-nparams #define degrees of freedom denominator \n",
    "    p = 1 - sp.stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic using CDF of F distribution (from scipy)\n",
    "    # small f -> large p\n",
    "    print(\"F-statistic and corresponding p-value:\", f, p)\n",
    "    \n",
    "    exes = numpy.linspace(0,3,30)\n",
    "    peas = [1 - scipy.stats.f.cdf(x, dfn, dfd) for x in exes]\n",
    "    plt.plot(exes,peas)\n",
    "    plt.axhline(y = 0.95, linestyle=\"--\", color=\"grey\")\n",
    "    plt.axhline(y = 0.05, linestyle=\"--\", color=\"grey\")\n",
    "    plt.axvline(x=f, color=\"red\")\n",
    "    plt.show()\n",
    "    # If the p-value is large (greater than α) then the first model is statistically better than the second.\n",
    "    # If the p-value is small (less than 1-α) then the second model is statistically better than the first.\n",
    "    if p > alpha:\n",
    "        print(\"Model 1 is better\")\n",
    "    elif p < (1-alpha):\n",
    "        print(\"Model 2 is better\")\n",
    "    else:\n",
    "        print(\"F-test is inconclusive (cannot reject H0)\")\n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_residuals = numpy.array([(yh - numpy.mean(y_test))**2 for yh in y_hats_test])\n",
    "# you can test your fit against a flat line to see if it does better than just taking the mean...\n",
    "f,p = f_test(sk_residuals, flat_residuals)\n",
    "print(f,p)\n",
    "\n",
    "# Hopefully we see that our diy residuals do worse that sklearn\n",
    "f,p = f_test(diy_residuals, sk_residuals)\n",
    "print(f,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- We generated a linear dataset (or loaded up a dataset) and manually split it into training and test sets\n",
    "- We implemented linear regression with gradient descent, using our own routine\n",
    "- We wrapped the routine in an estimator\n",
    "- We fitted the estimator and some of sklearn's own regression estimators, and compared them\n",
    "- We checked the RMSE and R2 scores\n",
    "- We implemented an F-test for models with the same number of parameters\n",
    "- We F-tested on the training data for significance of difference in the Sum of Sqd Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
